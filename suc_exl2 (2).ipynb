{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjYe6f4UQY-P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5M85zo7xQdNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alSoS86PQdQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFlS1zY_QdTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/turboderp/Llama-3.1-70B-Instruct-exl2"
      ],
      "metadata": {
        "id": "3li7dZr-TNUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MP3sqKGu4h-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c0b0cb-d561-45dd-b199-861374d5b9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 8145, done.\u001b[K\n",
            "remote: Counting objects: 100% (3398/3398), done.\u001b[K\n",
            "remote: Compressing objects: 100% (972/972), done.\u001b[K\n",
            "remote: Total 8145 (delta 2497), reused 2454 (delta 2426), pack-reused 4747 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8145/8145), 21.93 MiB | 12.44 MiB/s, done.\n",
            "Resolving deltas: 100% (5869/5869), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/turboderp/exllamav2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP4wtbKIQjlo",
        "outputId": "2d9ef52f-db82-4c3e-95b8-0597685158b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting exllamav2\n",
            "  Downloading exllamav2-0.2.8-py3-none-any.whl.metadata (467 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.2.2)\n",
            "Collecting ninja (from exllamav2)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting fastparquet (from exllamav2)\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from exllamav2) (14.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2024.11.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from exllamav2) (13.9.4)\n",
            "Requirement already satisfied: pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->exllamav2) (1.3.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->exllamav2) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->exllamav2) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->exllamav2) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->exllamav2) (3.0.2)\n",
            "Downloading exllamav2-0.2.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet, exllamav2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed exllamav2-0.2.8 fastparquet-2024.11.0 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd exllamav2\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Q0HXVip9Qsn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "liWLlEUMVkfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ibAYqFpEZA2",
        "outputId": "ba706ae4-4b30-4d4a-9517-dd2825f262bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfs0P6Hw8Vw5",
        "outputId": "0e7f8080-b091-412d-c9b3-3e290768314e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  8.0bpw https://huggingface.co/turboderp/TinyLlama-1B-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOOL4UWE8HeL",
        "outputId": "e758bf71-9595-480b-a262-270f78a37eff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TinyLlama-1B-exl2'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (82/82), 601.58 KiB | 3.18 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 1.07 GiB | 33.62 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  TinyLlama-1B-exl2 -p \"Once upon a time,\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT0ZM10u4yBJ",
        "outputId": "5ce37de1-8ed4-4721-a7fa-2a34873237a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: TinyLlama-1B-exl2\n",
            " -- Options: []\n",
            " -- Loading model...\n",
            " -- Loaded model in 1.3057 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Once upon a time, there was a man named John Smith. He was a sailor and had sailed the seas for many years. One day, he was on a ship with his wife with their two sons. He had a dream one night where he saw a mysterious island floating in the sky. He woke up and told his wife about it. The next morning, they set sail to find this island.\n",
            "In the middle of the ocean, they found a shipwreck and landed on an island. They climbed up the mountain, and there they found a city where they lived with the natives. They built a small\n",
            "\n",
            " -- Response generated in 1.15 seconds, 128 tokens, 111.55 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  TinyLlama-1B-exl2 -p \"1 + 1 = ,\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMjkLyvcRRK4",
        "outputId": "19825a05-7d51-44c1-e240-b35fae48105d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading exllamav2_ext extension (JIT)...\n",
            "\u001b[2KBuilding C++/CUDA extension \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:14:11\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Model: TinyLlama-1B-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: TinyLlama-1B-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 1.8843 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "1 + 1 = , 2 + 2 = , 3 + 3 = , 4 + 4 = , 5 + 5 = . 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.\n",
            "\n",
            " -- Response generated in 1.48 seconds, 128 tokens, 86.56 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  2.5bpw https://huggingface.co/turboderp/Llama-3.1-70B-Instruct-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHox2TXzRWcx",
        "outputId": "d3040b6e-ecb1-429f-f77f-3dcdab58d18f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama-3.1-70B-Instruct-exl2'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 76 (delta 14), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (76/76), 2.38 MiB | 1.62 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 6.64 GiB | 3.27 MiB/s, done.\n",
            "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
            "\toutput-00003-of-00003.safetensors\n",
            "\toutput-00001-of-00003.safetensors\n",
            "\toutput-00002-of-00003.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/turboderp/Llama-3.1-70B-Instruct-exl2/tree/2.5bpw"
      ],
      "metadata": {
        "id": "-T2FTuu6a-kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m  Llama-3.1-70B-Instruct-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPVaC5avR2BB",
        "outputId": "19410ad6-5149-4ee1-9222-8f1b7b28c2e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: Llama-3.1-70B-Instruct-exl2\n",
            " -- Options: []\n",
            "\u001b[2KTraceback (most recent call last):\n",
            "\u001b[2K  File \"/content/exllamav2/test_inference.py\", line 100, in <module>\n",
            "\u001b[2K    model, tokenizer = model_init.init(\n",
            "\u001b[2K                       ^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/model_init.py\", line 141, in init\n",
            "\u001b[2K    post_init_load(\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/model_init.py\", line 182, in post_init_load\n",
            "\u001b[2K    model.load(split, progress = progress)\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/model.py\", line 307, in load\n",
            "\u001b[2K    for item in f:\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/model.py\", line 335, in load_gen\n",
            "\u001b[2K    module.load()\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in \n",
            "decorate_context\n",
            "\u001b[2K    return func(*args, **kwargs)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/attn.py\", line 247, in load\n",
            "\u001b[2K    self.k_proj.load(device_context = device_context)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in \n",
            "decorate_context\n",
            "\u001b[2K    return func(*args, **kwargs)\n",
            "\u001b[2K           ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/linear.py\", line 127, in load\n",
            "\u001b[2K    if w is None: w = self.load_weight(cpu = output_map is not None)\n",
            "\u001b[2K                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/module.py\", line 119, in load_weight\n",
            "\u001b[2K    qtensors = self.load_multi(key, [\"q_weight\", \"q_invperm\", \"q_scale\", \"q_scale_max\", \"q_groups\", \n",
            "\"q_perm\", \"bias\"], cpu = cpu)\n",
            "\u001b[2K               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/module.py\", line 96, in load_multi\n",
            "\u001b[2K    tensors[k] = stfile.get_tensor(key + \".\" + k, device = self.device() if not cpu else \"cpu\")\n",
            "\u001b[2K                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2K  File \"/content/exllamav2/exllamav2/stloader.py\", line 157, in get_tensor\n",
            "\u001b[2K    tensor = torch.zeros(shape, dtype = dtype, device = device)\n",
            "\u001b[2K             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[2Ktorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity \n",
            "of 14.74 GiB of which 12.12 MiB is free. Process 172832 has 14.73 GiB memory in use. Of the \n",
            "allocated memory 14.40 GiB is allocated by PyTorch, and 197.03 MiB is reserved by PyTorch but \n",
            "unallocated. If reserved but unallocated memory is large try setting \n",
            "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for \n",
            "Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Loading: Llama-3.1-70B-Instruct-exl2 \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 69%\u001b[0m \u001b[33m0:01:36\u001b[0m \u001b[36m0:00:38\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYC4Eg-Ka_d7",
        "outputId": "0d7eed4e-1b54-497e-dcb2-35bd03e235a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            "usage: test_inference.py [-h] [-ed EVAL_DATASET] [-er EVAL_ROWS] [-el EVAL_LENGTH] [-et] [-e8]\n",
            "                         [-eq4] [-eq6] [-eq8] [-ecl] [-p PROMPT] [-pnb] [-t TOKENS] [-ps] [-s]\n",
            "                         [-mix MIX_LAYERS] [-nwu] [-sl] [-sp {wiki2}] [-rr RANK_REDUCE]\n",
            "                         [-mol MAX_OUTPUT_LEN] [-m MODEL_DIR] [-gs GPU_SPLIT] [-tp] [-l LENGTH]\n",
            "                         [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-ry ROPE_YARN] [-nfa] [-nxf] [-nsdpa]\n",
            "                         [-ng] [-lm] [-ept EXPERTS_PER_TOKEN] [-lq4] [-fst] [-ic]\n",
            "                         [-chunk CHUNK_SIZE]\n",
            "\n",
            "Test inference on ExLlamaV2 model\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -ed EVAL_DATASET, --eval_dataset EVAL_DATASET\n",
            "                        Perplexity evaluation dataset (.parquet file)\n",
            "  -er EVAL_ROWS, --eval_rows EVAL_ROWS\n",
            "                        Number of rows to apply from dataset (default 128)\n",
            "  -el EVAL_LENGTH, --eval_length EVAL_LENGTH\n",
            "                        Max no. tokens per sample\n",
            "  -et, --eval_token     Evaluate perplexity on token-by-token inference using cache\n",
            "  -e8, --eval_token_8bit\n",
            "                        Evaluate perplexity on token-by-token inference using 8-bit (FP8) cache\n",
            "  -eq4, --eval_token_q4\n",
            "                        Evaluate perplexity on token-by-token inference using Q4 cache\n",
            "  -eq6, --eval_token_q6\n",
            "                        Evaluate perplexity on token-by-token inference using Q6 cache\n",
            "  -eq8, --eval_token_q8\n",
            "                        Evaluate perplexity on token-by-token inference using Q8 cache\n",
            "  -ecl, --eval_context_lens\n",
            "                        Evaluate perplexity at range of context lengths\n",
            "  -p PROMPT, --prompt PROMPT\n",
            "                        Generate from prompt (basic sampling settings)\n",
            "  -pnb, --prompt_no_bos\n",
            "                        Don't add BOS token to prompt\n",
            "  -t TOKENS, --tokens TOKENS\n",
            "                        Max no. tokens\n",
            "  -ps, --prompt_speed   Test prompt processing (batch) speed over context length\n",
            "  -s, --speed           Test raw generation speed over context length\n",
            "  -mix MIX_LAYERS, --mix_layers MIX_LAYERS\n",
            "                        Load replacement layers from secondary model. Example: --mix_layers\n",
            "                        1,6-7:/mnt/models/other_model\n",
            "  -nwu, --no_warmup     Skip warmup before testing model\n",
            "  -sl, --stream_layers  Load model layer by layer (perplexity evaluation only)\n",
            "  -sp {wiki2}, --standard_perplexity {wiki2}\n",
            "                        Run standard (HF) perplexity test, stride 512 (experimental)\n",
            "  -rr RANK_REDUCE, --rank_reduce RANK_REDUCE\n",
            "                        Rank-reduction for MLP layers of model, in reverse order (for\n",
            "                        experimentation)\n",
            "  -mol MAX_OUTPUT_LEN, --max_output_len MAX_OUTPUT_LEN\n",
            "                        Set max output chunk size (incompatible with ppl tests)\n",
            "  -m MODEL_DIR, --model_dir MODEL_DIR\n",
            "                        Path to model directory\n",
            "  -gs GPU_SPLIT, --gpu_split GPU_SPLIT\n",
            "                        \"auto\", or VRAM allocation per GPU in GB. \"auto\" is implied by default in\n",
            "                        tensor-parallel mode.\n",
            "  -tp, --tensor_parallel\n",
            "                        Load in tensor-parallel mode (not fully supported for all models)\n",
            "  -l LENGTH, --length LENGTH\n",
            "                        Maximum sequence length\n",
            "  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n",
            "                        RoPE scaling factor\n",
            "  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n",
            "                        RoPE alpha value (NTK)\n",
            "  -ry ROPE_YARN, --rope_yarn ROPE_YARN\n",
            "                        Set RoPE YaRN factor (use default max_seq_len as\n",
            "                        original_max_position_embeddings if not configured)\n",
            "  -nfa, --no_flash_attn\n",
            "                        Disable Flash Attention\n",
            "  -nxf, --no_xformers   Disable xformers, an alternative plan of flash attn for older devices\n",
            "  -nsdpa, --no_sdpa     Disable Torch SDPA\n",
            "  -ng, --no_graphs      Disable Graphs\n",
            "  -lm, --low_mem        Enable VRAM optimizations, potentially trading off speed\n",
            "  -ept EXPERTS_PER_TOKEN, --experts_per_token EXPERTS_PER_TOKEN\n",
            "                        Override MoE model's default number of experts per token\n",
            "  -lq4, --load_q4       Load weights in Q4 mode\n",
            "  -fst, --fast_safetensors\n",
            "                        Deprecated (does nothing)\n",
            "  -ic, --ignore_compatibility\n",
            "                        Do not override model config options in case of compatibility issues\n",
            "  -chunk CHUNK_SIZE, --chunk_size CHUNK_SIZE\n",
            "                        Chunk size ('input length')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FF3Me8Uld7dn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}