{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_mpP_mxunH3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OY7P3TduyDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/turboderp/exllamav2\n",
        "!cd exllamav2 && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7uTT8N0uyGm",
        "outputId": "dac9e3b3-b57f-423c-c6c6-77068d592317"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 8145, done.\u001b[K\n",
            "remote: Counting objects: 100% (3373/3373), done.\u001b[K\n",
            "remote: Compressing objects: 100% (971/971), done.\u001b[K\n",
            "remote: Total 8145 (delta 2474), reused 2430 (delta 2402), pack-reused 4772 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8145/8145), 21.93 MiB | 16.42 MiB/s, done.\n",
            "Resolving deltas: 100% (5872/5872), done.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Collecting ninja (from -r requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (75.1.0)\n",
            "Collecting fastparquet (from -r requirements.txt (line 5))\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (14.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2024.11.6)\n",
            "Collecting numpy~=1.26.4 (from -r requirements.txt (line 12))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (13.9.4)\n",
            "Requirement already satisfied: pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2024.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->-r requirements.txt (line 13)) (0.28.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2025.1.31)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fastparquet-2024.11.0 ninja-1.11.1.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir my_model\n",
        "!huggingface-cli download turboderp/Mistral-7B-instruct-exl2 --revision 4.0bpw --local-dir my_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lNfQBxSu2AN",
        "outputId": "6408be3d-ecc0-4ab1-e0fd-3a7a2822f981"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading 'config.json' to 'my_model/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.59513ed03ca30cc68cde30bd4376c4f9c42077d1.incomplete'\n",
            "Downloading 'generation_config.json' to 'my_model/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.2c5f418036a121b3fd432d1bf2b3c5c9daf59fab.incomplete'\n",
            "Downloading '.gitattributes' to 'my_model/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'README.md' to 'my_model/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.3ae711baab2b51675bb05cde6bbf8759e31d836b.incomplete'\n",
            "Downloading 'special_tokens_map.json' to 'my_model/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.a52c50a199269393cd1548c7e6a77a654bd2001b.incomplete'\n",
            "Downloading 'tokenizer.json' to 'my_model/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.86fcc6c0e796ce8cc331b36ccb8ebc7a1c3ed1e0.incomplete'\n",
            "\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.28MB/s]\n",
            "Download complete. Moving file to my_model/config.json\n",
            "\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 1.18MB/s]\n",
            "Download complete. Moving file to my_model/generation_config.json\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 12.4MB/s]\n",
            "Download complete. Moving file to my_model/.gitattributes\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:04,  1.94it/s]\n",
            "README.md: 100% 2.21k/2.21k [00:00<00:00, 19.9MB/s]\n",
            "Download complete. Moving file to my_model/README.md\n",
            "\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 844kB/s]\n",
            "Download complete. Moving file to my_model/special_tokens_map.json\n",
            "\n",
            "tokenizer.json:   0% 0.00/1.80M [00:00<?, ?B/s]\u001b[ADownloading 'pytorch_model.bin.index.json' to 'my_model/.cache/huggingface/download/8TePU6wZ6PO52hlgkfCnTYnlMSI=.520ee2fc9a8659d53be1f9f0a4502c151fc24775.incomplete'\n",
            "Downloading 'tokenizer.model' to 'my_model/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055.incomplete'\n",
            "Downloading 'tokenizer_config.json' to 'my_model/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.c80b76077e4f946fb80bed93e4c3bed245ff8604.incomplete'\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 41.2MB/s]\n",
            "Download complete. Moving file to my_model/tokenizer.model\n",
            "\n",
            "\n",
            "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 33.3MB/s]\n",
            "Download complete. Moving file to my_model/pytorch_model.bin.index.json\n",
            "\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 4.27MB/s]\n",
            "Download complete. Moving file to my_model/tokenizer.json\n",
            "\n",
            "tokenizer_config.json: 100% 963/963 [00:00<00:00, 10.8MB/s]\n",
            "Download complete. Moving file to my_model/tokenizer_config.json\n",
            "Downloading 'output.safetensors' to 'my_model/.cache/huggingface/download/2regD2SwVb9sZxeoA_WsW5_KDvM=.4bc49294c8e28eaf600081fb0d313b0624e943a7ca5e982d94e25c60cc08ed07.incomplete'\n",
            "\n",
            "output.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "output.safetensors:   1% 21.0M/3.86G [00:00<00:20, 189MB/s]\u001b[A\n",
            "output.safetensors:   1% 52.4M/3.86G [00:00<00:17, 220MB/s]\u001b[A\n",
            "output.safetensors:   2% 83.9M/3.86G [00:00<00:16, 223MB/s]\u001b[A\n",
            "output.safetensors:   3% 115M/3.86G [00:00<00:16, 225MB/s] \u001b[A\n",
            "output.safetensors:   4% 147M/3.86G [00:00<00:16, 223MB/s]\u001b[A\n",
            "output.safetensors:   5% 178M/3.86G [00:00<00:16, 218MB/s]\u001b[A\n",
            "output.safetensors:   5% 210M/3.86G [00:00<00:16, 221MB/s]\u001b[A\n",
            "output.safetensors:   6% 241M/3.86G [00:01<00:16, 223MB/s]\u001b[A\n",
            "output.safetensors:   7% 273M/3.86G [00:01<00:17, 209MB/s]\u001b[A\n",
            "output.safetensors:   8% 304M/3.86G [00:01<00:16, 213MB/s]\u001b[A\n",
            "output.safetensors:   9% 336M/3.86G [00:01<00:15, 223MB/s]\u001b[A\n",
            "output.safetensors:  10% 367M/3.86G [00:01<00:15, 224MB/s]\u001b[A\n",
            "output.safetensors:  10% 398M/3.86G [00:01<00:15, 229MB/s]\u001b[A\n",
            "output.safetensors:  11% 430M/3.86G [00:01<00:14, 229MB/s]\u001b[A\n",
            "output.safetensors:  12% 461M/3.86G [00:02<00:15, 222MB/s]\u001b[A\n",
            "output.safetensors:  13% 493M/3.86G [00:02<00:15, 219MB/s]\u001b[A\n",
            "output.safetensors:  14% 524M/3.86G [00:02<00:15, 221MB/s]\u001b[A\n",
            "output.safetensors:  14% 556M/3.86G [00:02<00:14, 226MB/s]\u001b[A\n",
            "output.safetensors:  15% 587M/3.86G [00:02<00:14, 226MB/s]\u001b[A\n",
            "output.safetensors:  16% 619M/3.86G [00:02<00:15, 214MB/s]\u001b[A\n",
            "output.safetensors:  17% 650M/3.86G [00:02<00:15, 207MB/s]\u001b[A\n",
            "output.safetensors:  17% 671M/3.86G [00:03<00:16, 198MB/s]\u001b[A\n",
            "output.safetensors:  18% 692M/3.86G [00:03<00:16, 195MB/s]\u001b[A\n",
            "output.safetensors:  18% 713M/3.86G [00:03<00:16, 190MB/s]\u001b[A\n",
            "output.safetensors:  19% 744M/3.86G [00:03<00:14, 209MB/s]\u001b[A\n",
            "output.safetensors:  20% 765M/3.86G [00:03<00:15, 204MB/s]\u001b[A\n",
            "output.safetensors:  21% 797M/3.86G [00:03<00:14, 212MB/s]\u001b[A\n",
            "output.safetensors:  21% 828M/3.86G [00:03<00:16, 183MB/s]\u001b[A\n",
            "output.safetensors:  22% 860M/3.86G [00:04<00:15, 195MB/s]\u001b[A\n",
            "output.safetensors:  23% 891M/3.86G [00:04<00:14, 203MB/s]\u001b[A\n",
            "output.safetensors:  24% 912M/3.86G [00:04<00:15, 195MB/s]\u001b[A\n",
            "output.safetensors:  24% 933M/3.86G [00:04<00:14, 198MB/s]\u001b[A\n",
            "output.safetensors:  25% 954M/3.86G [00:04<00:14, 199MB/s]\u001b[A\n",
            "output.safetensors:  25% 975M/3.86G [00:04<00:14, 201MB/s]\u001b[A\n",
            "output.safetensors:  26% 996M/3.86G [00:04<00:14, 200MB/s]\u001b[A\n",
            "output.safetensors:  26% 1.02G/3.86G [00:04<00:14, 199MB/s]\u001b[A\n",
            "output.safetensors:  27% 1.05G/3.86G [00:04<00:13, 209MB/s]\u001b[A\n",
            "output.safetensors:  28% 1.07G/3.86G [00:05<00:13, 200MB/s]\u001b[A\n",
            "output.safetensors:  29% 1.10G/3.86G [00:05<00:13, 209MB/s]\u001b[A\n",
            "output.safetensors:  29% 1.13G/3.86G [00:05<00:12, 212MB/s]\u001b[A\n",
            "output.safetensors:  30% 1.16G/3.86G [00:05<00:13, 202MB/s]\u001b[A\n",
            "output.safetensors:  31% 1.18G/3.86G [00:05<00:13, 202MB/s]\u001b[A\n",
            "output.safetensors:  31% 1.21G/3.86G [00:05<00:13, 202MB/s]\u001b[A\n",
            "output.safetensors:  32% 1.24G/3.86G [00:05<00:12, 208MB/s]\u001b[A\n",
            "output.safetensors:  33% 1.27G/3.86G [00:06<00:12, 206MB/s]\u001b[A\n",
            "output.safetensors:  34% 1.30G/3.86G [00:06<00:11, 217MB/s]\u001b[A\n",
            "output.safetensors:  34% 1.33G/3.86G [00:06<00:11, 222MB/s]\u001b[A\n",
            "output.safetensors:  35% 1.36G/3.86G [00:06<00:11, 224MB/s]\u001b[A\n",
            "output.safetensors:  36% 1.39G/3.86G [00:06<00:11, 214MB/s]\u001b[A\n",
            "output.safetensors:  37% 1.43G/3.86G [00:06<00:11, 206MB/s]\u001b[A\n",
            "output.safetensors:  38% 1.46G/3.86G [00:06<00:11, 214MB/s]\u001b[A\n",
            "output.safetensors:  39% 1.49G/3.86G [00:07<00:11, 200MB/s]\u001b[A\n",
            "output.safetensors:  39% 1.51G/3.86G [00:07<00:11, 197MB/s]\u001b[A\n",
            "output.safetensors:  40% 1.53G/3.86G [00:07<00:12, 184MB/s]\u001b[A\n",
            "output.safetensors:  40% 1.55G/3.86G [00:07<00:13, 175MB/s]\u001b[A\n",
            "output.safetensors:  41% 1.57G/3.86G [00:07<00:12, 176MB/s]\u001b[A\n",
            "output.safetensors:  42% 1.60G/3.86G [00:08<00:18, 121MB/s]\u001b[A\n",
            "output.safetensors:  42% 1.64G/3.86G [00:08<00:14, 149MB/s]\u001b[A\n",
            "output.safetensors:  43% 1.66G/3.86G [00:09<00:40, 54.1MB/s]\u001b[A\n",
            "output.safetensors:  44% 1.69G/3.86G [00:09<00:29, 73.9MB/s]\u001b[A\n",
            "output.safetensors:  45% 1.72G/3.86G [00:09<00:22, 96.3MB/s]\u001b[A\n",
            "output.safetensors:  45% 1.74G/3.86G [00:09<00:19, 110MB/s] \u001b[A\n",
            "output.safetensors:  46% 1.77G/3.86G [00:09<00:15, 135MB/s]\u001b[A\n",
            "output.safetensors:  47% 1.80G/3.86G [00:09<00:13, 154MB/s]\u001b[A\n",
            "output.safetensors:  48% 1.84G/3.86G [00:10<00:12, 156MB/s]\u001b[A\n",
            "output.safetensors:  48% 1.86G/3.86G [00:10<00:16, 122MB/s]\u001b[A\n",
            "output.safetensors:  49% 1.88G/3.86G [00:10<00:19, 99.8MB/s]\u001b[A\n",
            "output.safetensors:  49% 1.90G/3.86G [00:11<00:23, 83.2MB/s]\u001b[A\n",
            "output.safetensors:  50% 1.92G/3.86G [00:11<00:21, 91.8MB/s]\u001b[A\n",
            "output.safetensors:  50% 1.94G/3.86G [00:11<00:20, 95.8MB/s]\u001b[A\n",
            "output.safetensors:  51% 1.96G/3.86G [00:11<00:23, 79.6MB/s]\u001b[A\n",
            "output.safetensors:  51% 1.97G/3.86G [00:11<00:24, 76.4MB/s]\u001b[A\n",
            "output.safetensors:  51% 1.98G/3.86G [00:12<00:25, 74.4MB/s]\u001b[A\n",
            "output.safetensors:  52% 1.99G/3.86G [00:12<00:27, 68.9MB/s]\u001b[A\n",
            "output.safetensors:  52% 2.00G/3.86G [00:12<00:27, 68.5MB/s]\u001b[A\n",
            "output.safetensors:  52% 2.01G/3.86G [00:12<00:27, 67.0MB/s]\u001b[A\n",
            "output.safetensors:  52% 2.02G/3.86G [00:12<00:30, 60.2MB/s]\u001b[A\n",
            "output.safetensors:  53% 2.03G/3.86G [00:13<00:29, 61.1MB/s]\u001b[A\n",
            "output.safetensors:  53% 2.04G/3.86G [00:13<00:29, 61.0MB/s]\u001b[A\n",
            "output.safetensors:  54% 2.08G/3.86G [00:13<00:17, 103MB/s] \u001b[A\n",
            "output.safetensors:  55% 2.11G/3.86G [00:13<00:13, 131MB/s]\u001b[A\n",
            "output.safetensors:  55% 2.13G/3.86G [00:13<00:13, 131MB/s]\u001b[A\n",
            "output.safetensors:  56% 2.15G/3.86G [00:13<00:11, 145MB/s]\u001b[A\n",
            "output.safetensors:  56% 2.18G/3.86G [00:13<00:10, 164MB/s]\u001b[A\n",
            "output.safetensors:  57% 2.21G/3.86G [00:14<00:08, 186MB/s]\u001b[A\n",
            "output.safetensors:  58% 2.23G/3.86G [00:14<00:09, 171MB/s]\u001b[A\n",
            "output.safetensors:  59% 2.26G/3.86G [00:14<00:08, 183MB/s]\u001b[A\n",
            "output.safetensors:  59% 2.29G/3.86G [00:14<00:09, 166MB/s]\u001b[A\n",
            "output.safetensors:  60% 2.31G/3.86G [00:14<00:09, 167MB/s]\u001b[A\n",
            "output.safetensors:  61% 2.34G/3.86G [00:14<00:08, 180MB/s]\u001b[A\n",
            "output.safetensors:  61% 2.37G/3.86G [00:14<00:08, 185MB/s]\u001b[A\n",
            "output.safetensors:  62% 2.40G/3.86G [00:15<00:07, 203MB/s]\u001b[A\n",
            "output.safetensors:  63% 2.43G/3.86G [00:15<00:07, 204MB/s]\u001b[A\n",
            "output.safetensors:  64% 2.46G/3.86G [00:15<00:07, 199MB/s]\u001b[A\n",
            "output.safetensors:  65% 2.50G/3.86G [00:15<00:06, 221MB/s]\u001b[A\n",
            "output.safetensors:  65% 2.53G/3.86G [00:15<00:05, 236MB/s]\u001b[A\n",
            "output.safetensors:  66% 2.56G/3.86G [00:15<00:05, 238MB/s]\u001b[A\n",
            "output.safetensors:  67% 2.59G/3.86G [00:15<00:05, 234MB/s]\u001b[A\n",
            "output.safetensors:  68% 2.62G/3.86G [00:16<00:05, 229MB/s]\u001b[A\n",
            "output.safetensors:  69% 2.65G/3.86G [00:16<00:05, 229MB/s]\u001b[A\n",
            "output.safetensors:  70% 2.68G/3.86G [00:16<00:05, 221MB/s]\u001b[A\n",
            "output.safetensors:  70% 2.72G/3.86G [00:16<00:05, 222MB/s]\u001b[A\n",
            "output.safetensors:  71% 2.75G/3.86G [00:18<00:21, 53.0MB/s]\u001b[A\n",
            "output.safetensors:  72% 2.77G/3.86G [00:18<00:17, 61.0MB/s]\u001b[A\n",
            "output.safetensors:  73% 2.80G/3.86G [00:18<00:13, 80.3MB/s]\u001b[A\n",
            "output.safetensors:  73% 2.83G/3.86G [00:18<00:10, 100MB/s] \u001b[A\n",
            "output.safetensors:  74% 2.86G/3.86G [00:18<00:08, 124MB/s]\u001b[A\n",
            "output.safetensors:  75% 2.89G/3.86G [00:18<00:06, 142MB/s]\u001b[A\n",
            "output.safetensors:  76% 2.93G/3.86G [00:18<00:05, 160MB/s]\u001b[A\n",
            "output.safetensors:  77% 2.96G/3.86G [00:19<00:05, 176MB/s]\u001b[A\n",
            "output.safetensors:  77% 2.99G/3.86G [00:19<00:04, 189MB/s]\u001b[A\n",
            "output.safetensors:  78% 3.02G/3.86G [00:19<00:04, 199MB/s]\u001b[A\n",
            "output.safetensors:  79% 3.05G/3.86G [00:19<00:03, 213MB/s]\u001b[A\n",
            "output.safetensors:  80% 3.08G/3.86G [00:19<00:03, 219MB/s]\u001b[A\n",
            "output.safetensors:  81% 3.11G/3.86G [00:19<00:03, 221MB/s]\u001b[A\n",
            "output.safetensors:  81% 3.15G/3.86G [00:19<00:03, 227MB/s]\u001b[A\n",
            "output.safetensors:  82% 3.18G/3.86G [00:20<00:02, 230MB/s]\u001b[A\n",
            "output.safetensors:  83% 3.21G/3.86G [00:20<00:02, 231MB/s]\u001b[A\n",
            "output.safetensors:  84% 3.24G/3.86G [00:20<00:02, 232MB/s]\u001b[A\n",
            "output.safetensors:  85% 3.27G/3.86G [00:20<00:02, 228MB/s]\u001b[A\n",
            "output.safetensors:  86% 3.30G/3.86G [00:20<00:02, 221MB/s]\u001b[A\n",
            "output.safetensors:  86% 3.33G/3.86G [00:20<00:02, 224MB/s]\u001b[A\n",
            "output.safetensors:  87% 3.37G/3.86G [00:20<00:02, 227MB/s]\u001b[A\n",
            "output.safetensors:  88% 3.40G/3.86G [00:21<00:02, 224MB/s]\u001b[A\n",
            "output.safetensors:  89% 3.43G/3.86G [00:21<00:01, 222MB/s]\u001b[A\n",
            "output.safetensors:  90% 3.46G/3.86G [00:21<00:01, 219MB/s]\u001b[A\n",
            "output.safetensors:  90% 3.49G/3.86G [00:21<00:01, 226MB/s]\u001b[A\n",
            "output.safetensors:  91% 3.52G/3.86G [00:21<00:02, 161MB/s]\u001b[A\n",
            "output.safetensors:  92% 3.55G/3.86G [00:21<00:01, 180MB/s]\u001b[A\n",
            "output.safetensors:  93% 3.58G/3.86G [00:21<00:01, 182MB/s]\u001b[A\n",
            "output.safetensors:  93% 3.61G/3.86G [00:22<00:01, 193MB/s]\u001b[A\n",
            "output.safetensors:  94% 3.64G/3.86G [00:22<00:01, 205MB/s]\u001b[A\n",
            "output.safetensors:  95% 3.67G/3.86G [00:22<00:00, 212MB/s]\u001b[A\n",
            "output.safetensors:  96% 3.70G/3.86G [00:22<00:00, 214MB/s]\u001b[A\n",
            "output.safetensors:  97% 3.73G/3.86G [00:22<00:00, 214MB/s]\u001b[A\n",
            "output.safetensors:  97% 3.76G/3.86G [00:22<00:00, 218MB/s]\u001b[A\n",
            "output.safetensors:  98% 3.80G/3.86G [00:22<00:00, 222MB/s]\u001b[A\n",
            "output.safetensors:  99% 3.83G/3.86G [00:23<00:00, 218MB/s]\u001b[A\n",
            "output.safetensors: 100% 3.86G/3.86G [00:23<00:00, 166MB/s]\n",
            "Download complete. Moving file to my_model/output.safetensors\n",
            "Fetching 10 files: 100% 10/10 [00:24<00:00,  2.44s/it]\n",
            "/content/my_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q_yJqznu4Tu",
        "outputId": "f5496d5a-03d8-4181-b662-c9de0e9142fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: []\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            " -- Loading model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/examples/chat.py\", line 159, in <module>\n",
            "    model_init.post_init_load(model, args, allow_auto_split = True)\n",
            "  File \"/content/exllamav2/exllamav2/model_init.py\", line 182, in post_init_load\n",
            "    model.load(split, progress = progress)\n",
            "  File \"/content/exllamav2/exllamav2/model.py\", line 307, in load\n",
            "    for item in f:\n",
            "  File \"/content/exllamav2/exllamav2/model.py\", line 335, in load_gen\n",
            "    module.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/mlp.py\", line 158, in load\n",
            "    if self.gate_proj is not None: self.gate_proj.load(device_context = device_context, output_map = down_map)\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/linear.py\", line 156, in load\n",
            "    ext_c.tensor_remap_4bit(w[\"q_scale\"], output_map)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlKsAdYoynCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ù†Ù…ÙˆØ°Ø¬ **ExLlamaV2 (exl2)** Ù…ØµÙ…Ù… Ù„ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ù…Ø«Ù„ `HF Transformers`ØŒ ÙˆÙŠØ³ØªÙ‡Ù„Ùƒ Ø°Ø§ÙƒØ±Ø© VRAM Ø£Ù‚Ù„ Ø¨Ø³Ø¨Ø¨ Ø¶ØºØ· Ø§Ù„ÙˆØ²Ù† ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¹Ø§Ù„Ø©.  \n",
        "\n",
        "### **Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© (VRAM) Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø¶ØºØ·:**\n",
        "#### **Llama 3.1 - 8B Instruct (exl2)**\n",
        "| **Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¶ØºØ·** | **Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ VRAM** (ØªÙ‚Ø±ÙŠØ¨ÙŠ) |\n",
        "|--------------|----------------|\n",
        "| 4-bit (exl2) | ~7-9 GB |\n",
        "| 3-bit (exl2) | ~5-7 GB |\n",
        "| 2-bit (exl2) | ~4-5 GB |\n",
        "\n",
        "#### **Llama 3.1 - 70B (exl2)**\n",
        "| **Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¶ØºØ·** | **Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ VRAM** (ØªÙ‚Ø±ÙŠØ¨ÙŠ) |\n",
        "|--------------|----------------|\n",
        "| 4-bit (exl2) | ~35-40 GB |\n",
        "| 3-bit (exl2) | ~24-30 GB |\n",
        "| 2-bit (exl2) | ~18-22 GB |\n",
        "\n",
        "### **ÙƒÙŠÙ ØªØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù€ VRAMØŸ**\n",
        "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Ø¹Ø±Ø¶ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù€ GPU\n",
        "print(f\"VRAM Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"VRAM Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "```\n",
        "\n",
        "### **Ø£ÙØ¶Ù„ GPU Ù„ØªØ´ØºÙŠÙ„ ExLlamaV2**\n",
        "- **8GB VRAM** â†’ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ `8B (exl2 4-bit)` Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§Ø­Ø© ÙƒØ§ÙÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·ÙˆÙŠÙ„.\n",
        "- **12GB VRAM (Ù…Ø«Ù„ RTX 3060)** â†’ Ø¬ÙŠØ¯ Ù„Ù€ `8B (exl2 4-bit)` Ù…Ø¹ Ø£Ø¯Ø§Ø¡ Ù…Ø¹Ù‚ÙˆÙ„.\n",
        "- **24GB VRAM (Ù…Ø«Ù„ RTX 4090 Ø£Ùˆ A6000)** â†’ ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ `8B (exl2 3-bit)` Ø£Ùˆ `70B (exl2 4-bit)`.\n",
        "- **48GB+ VRAM (A100, H100, MI300X)** â†’ Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ù…Ø«Ù„ `70B (exl2 3-bit)`.\n",
        "\n",
        "### **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬**\n",
        "- **8B (exl2 4-bit) ÙŠØ­ØªØ§Ø¬ 7-9GB VRAM**.\n",
        "- Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ **RTX 3060 (12GB)** Ø£Ùˆ Ø£Ø¹Ù„Ù‰ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø³Ù„Ø§Ø³Ø©.\n",
        "- ÙƒÙ„Ù…Ø§ ÙƒØ§Ù†Øª **Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¶ØºØ· Ø£Ù‚Ù„** (Ù…Ø«Ù„ 3-bit Ø£Ùˆ 2-bit)ØŒ ÙƒØ§Ù† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ù‚Ù„ Ù„ÙƒÙ†Ù‡ Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø©.\n",
        "\n",
        "Ù‡Ù„ Ù„Ø¯ÙŠÙƒ GPU Ù…Ø¹ÙŠÙ† ØªØ±ÙŠØ¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„ÙŠÙ‡ØŸ ğŸš€"
      ],
      "metadata": {
        "id": "2MTJN6mrylyv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flWoVR71z-AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HC_uKqQ5z99e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/my_model"
      ],
      "metadata": {
        "id": "2dYqLbjXz96K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!mkdir my_model\n",
        "!huggingface-cli download turboderp/Llama-3.1-8B-Instruct-exl2 --revision 3.0bpw --local-dir my_model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22hQyf_zz92R",
        "outputId": "df9a2b1a-1ae3-4806-cbf4-99e2586638ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 9 files:   0% 0/9 [00:00<?, ?it/s]Downloading 'generation_config.json' to 'my_model/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.35c6e28b5b0793bd1ca758e2688c7963de121018.incomplete'\n",
            "Downloading 'model.safetensors.index.json' to 'my_model/.cache/huggingface/download/yVzAsSxRSINSz-tQbpx-TLpfkLU=.0fd8120f1c6acddc268ebc2583058efaf699a771.incomplete'\n",
            "Downloading 'special_tokens_map.json' to 'my_model/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.d8cd5076496dbe4be2320312abc10adc43097b81.incomplete'\n",
            "Downloading 'README.md' to 'my_model/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.5eee24d3b8d52f22cd64c022f7b3f107488de3fb.incomplete'\n",
            "Downloading 'output.safetensors' to 'my_model/.cache/huggingface/download/2regD2SwVb9sZxeoA_WsW5_KDvM=.47cdd1fcd5ad66074090afed24ff2ea608b707b10c5baac5d54c0d6233e6210d.incomplete'\n",
            "Downloading '.gitattributes' to 'my_model/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'config.json' to 'my_model/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.06d1a629305421c4394f1d13c1cc173d37d9d8ae.incomplete'\n",
            "Downloading 'tokenizer.json' to 'my_model/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.94eacd0897072dcd7b84d1f6ff3c3f6d1933a8cc.incomplete'\n",
            "\n",
            "generation_config.json: 100% 144/144 [00:00<00:00, 835kB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 654kB/s]\n",
            "Download complete. Moving file to my_model/generation_config.json\n",
            "Download complete. Moving file to my_model/special_tokens_map.json\n",
            "\n",
            "README.md: 100% 139/139 [00:00<00:00, 419kB/s]\n",
            "\n",
            "model.safetensors.index.json:   0% 0.00/23.9k [00:00<?, ?B/s]\u001b[ADownload complete. Moving file to my_model/README.md\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 41.4MB/s]\n",
            "Download complete. Moving file to my_model/model.safetensors.index.json\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 5.04MB/s]\n",
            "\n",
            "config.json:   0% 0.00/1.21k [00:00<?, ?B/s]\u001b[ADownload complete. Moving file to my_model/.gitattributes\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 9.18MB/s]\n",
            "Download complete. Moving file to my_model/config.json\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:04,  1.93it/s]\n",
            "tokenizer.json:   0% 0.00/9.08M [00:00<?, ?B/s]\u001b[ADownloading 'tokenizer_config.json' to 'my_model/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.bca09bd29e5b2ab1cd77fa23e58678a3f12e88e6.incomplete'\n",
            "\n",
            "\n",
            "output.safetensors:   0% 0.00/4.08G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 50.9k/50.9k [00:00<00:00, 35.4MB/s]\n",
            "Download complete. Moving file to my_model/tokenizer_config.json\n",
            "\n",
            "tokenizer.json: 100% 9.08M/9.08M [00:00<00:00, 13.7MB/s]\n",
            "Download complete. Moving file to my_model/tokenizer.json\n",
            "\n",
            "\n",
            "output.safetensors:   0% 10.5M/4.08G [00:00<05:43, 11.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   1% 21.0M/4.08G [00:01<04:01, 16.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   1% 31.5M/4.08G [00:01<03:28, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   1% 41.9M/4.08G [00:02<03:11, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   1% 52.4M/4.08G [00:02<03:02, 22.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   2% 62.9M/4.08G [00:03<02:56, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   2% 73.4M/4.08G [00:03<03:17, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   2% 83.9M/4.08G [00:04<03:07, 21.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   2% 94.4M/4.08G [00:04<03:00, 22.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   3% 105M/4.08G [00:05<02:55, 22.7MB/s] \u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   3% 115M/4.08G [00:05<02:52, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   3% 126M/4.08G [00:05<02:49, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   3% 136M/4.08G [00:06<02:47, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   4% 147M/4.08G [00:06<02:45, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   4% 157M/4.08G [00:07<02:44, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   4% 168M/4.08G [00:07<02:43, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   4% 178M/4.08G [00:08<02:43, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   5% 189M/4.08G [00:08<02:42, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   5% 199M/4.08G [00:08<02:41, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   5% 210M/4.08G [00:09<02:41, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   5% 220M/4.08G [00:09<02:40, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   6% 231M/4.08G [00:10<02:40, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   6% 241M/4.08G [00:10<02:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   6% 252M/4.08G [00:11<02:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   6% 262M/4.08G [00:11<02:38, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   7% 273M/4.08G [00:11<02:38, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   7% 283M/4.08G [00:12<02:37, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   7% 294M/4.08G [00:12<02:37, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   7% 304M/4.08G [00:13<02:38, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   8% 315M/4.08G [00:13<02:36, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   8% 325M/4.08G [00:14<02:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   8% 336M/4.08G [00:14<02:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   8% 346M/4.08G [00:15<02:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   9% 357M/4.08G [00:15<02:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   9% 367M/4.08G [00:15<02:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   9% 377M/4.08G [00:16<02:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:   9% 388M/4.08G [00:16<02:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  10% 398M/4.08G [00:17<02:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  10% 409M/4.08G [00:17<02:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  10% 419M/4.08G [00:18<02:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  11% 430M/4.08G [00:18<02:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  11% 440M/4.08G [00:18<02:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  11% 451M/4.08G [00:19<02:30, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  11% 461M/4.08G [00:19<02:31, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  12% 472M/4.08G [00:20<02:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  12% 482M/4.08G [00:20<02:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  12% 493M/4.08G [00:21<02:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  12% 503M/4.08G [00:21<02:29, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  13% 514M/4.08G [00:21<02:27, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  13% 524M/4.08G [00:22<02:27, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  13% 535M/4.08G [00:22<02:27, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  13% 545M/4.08G [00:23<02:26, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  14% 556M/4.08G [00:23<02:26, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  14% 566M/4.08G [00:24<02:25, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  14% 577M/4.08G [00:24<02:25, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  14% 587M/4.08G [00:25<02:25, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  15% 598M/4.08G [00:25<02:24, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  15% 608M/4.08G [00:25<02:24, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  15% 619M/4.08G [00:26<02:24, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  15% 629M/4.08G [00:26<02:23, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  16% 640M/4.08G [00:27<02:23, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  16% 650M/4.08G [00:27<02:22, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  16% 661M/4.08G [00:28<02:22, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  16% 671M/4.08G [00:28<02:21, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  17% 682M/4.08G [00:28<02:21, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  17% 692M/4.08G [00:29<02:20, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  17% 703M/4.08G [00:29<02:20, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  17% 713M/4.08G [00:30<02:20, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  18% 724M/4.08G [00:30<02:19, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  18% 734M/4.08G [00:31<02:19, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  18% 744M/4.08G [00:31<02:19, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  18% 755M/4.08G [00:32<02:18, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  19% 765M/4.08G [00:32<02:18, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  19% 776M/4.08G [00:32<02:17, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  19% 786M/4.08G [00:33<02:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  20% 797M/4.08G [00:33<02:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  20% 807M/4.08G [00:34<02:16, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  20% 818M/4.08G [00:34<02:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  20% 828M/4.08G [00:35<02:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  21% 839M/4.08G [00:35<02:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  21% 849M/4.08G [00:35<02:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  21% 860M/4.08G [00:36<02:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  21% 870M/4.08G [00:36<02:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  22% 881M/4.08G [00:37<02:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  22% 891M/4.08G [00:37<02:12, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  22% 902M/4.08G [00:38<02:12, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  22% 912M/4.08G [00:38<02:11, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  23% 923M/4.08G [00:38<02:11, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  23% 933M/4.08G [00:39<02:10, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  23% 944M/4.08G [00:39<02:10, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  23% 954M/4.08G [00:40<02:10, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  24% 965M/4.08G [00:40<02:10, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  24% 975M/4.08G [00:41<02:08, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  24% 986M/4.08G [00:41<02:08, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  24% 996M/4.08G [00:42<02:07, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  25% 1.01G/4.08G [00:42<02:07, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  25% 1.02G/4.08G [00:42<02:07, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  25% 1.03G/4.08G [00:43<02:07, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  25% 1.04G/4.08G [00:43<02:06, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  26% 1.05G/4.08G [00:44<02:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  26% 1.06G/4.08G [00:44<02:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  26% 1.07G/4.08G [00:45<02:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  26% 1.08G/4.08G [00:45<02:04, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  27% 1.09G/4.08G [00:45<02:04, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  27% 1.10G/4.08G [00:46<02:04, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  27% 1.11G/4.08G [00:46<02:06, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  27% 1.12G/4.08G [00:47<02:05, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  28% 1.13G/4.08G [00:47<02:04, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  28% 1.14G/4.08G [00:48<02:03, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  28% 1.15G/4.08G [00:48<02:02, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  28% 1.16G/4.08G [00:49<02:02, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  29% 1.17G/4.08G [00:49<02:01, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  29% 1.18G/4.08G [00:49<02:00, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  29% 1.20G/4.08G [00:50<02:00, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  30% 1.21G/4.08G [00:50<01:59, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  30% 1.22G/4.08G [00:51<01:59, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  30% 1.23G/4.08G [00:51<01:58, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  30% 1.24G/4.08G [00:52<01:58, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  31% 1.25G/4.08G [00:52<01:57, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  31% 1.26G/4.08G [00:52<01:57, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  31% 1.27G/4.08G [00:53<01:56, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  31% 1.28G/4.08G [00:53<01:56, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  32% 1.29G/4.08G [00:54<01:56, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  32% 1.30G/4.08G [00:54<01:55, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  32% 1.31G/4.08G [00:55<01:55, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  32% 1.32G/4.08G [00:55<01:54, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  33% 1.33G/4.08G [00:56<01:54, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  33% 1.34G/4.08G [00:56<01:53, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  33% 1.35G/4.08G [00:56<01:53, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  33% 1.36G/4.08G [00:57<01:53, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  34% 1.37G/4.08G [00:57<01:52, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  34% 1.38G/4.08G [00:58<01:52, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  34% 1.39G/4.08G [00:58<01:51, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  34% 1.41G/4.08G [00:59<01:51, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  35% 1.42G/4.08G [00:59<02:02, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  35% 1.43G/4.08G [01:00<01:58, 22.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  35% 1.44G/4.08G [01:00<01:55, 22.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  35% 1.45G/4.08G [01:00<01:53, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  36% 1.46G/4.08G [01:01<01:51, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  36% 1.47G/4.08G [01:01<01:50, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  36% 1.48G/4.08G [01:02<01:49, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  36% 1.49G/4.08G [01:02<01:48, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  37% 1.50G/4.08G [01:03<01:48, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  37% 1.51G/4.08G [01:03<01:47, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  37% 1.52G/4.08G [01:04<01:46, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  37% 1.53G/4.08G [01:04<01:46, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  38% 1.54G/4.08G [01:04<01:45, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  38% 1.55G/4.08G [01:05<01:45, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  38% 1.56G/4.08G [01:05<01:44, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  39% 1.57G/4.08G [01:06<01:44, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  39% 1.58G/4.08G [01:06<01:43, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  39% 1.59G/4.08G [01:07<01:43, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  39% 1.60G/4.08G [01:07<01:43, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  40% 1.61G/4.08G [01:07<01:42, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  40% 1.63G/4.08G [01:08<01:42, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  40% 1.64G/4.08G [01:08<01:41, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  40% 1.65G/4.08G [01:09<01:41, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  41% 1.66G/4.08G [01:09<01:40, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  41% 1.67G/4.08G [01:10<01:41, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  41% 1.68G/4.08G [01:10<01:40, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  41% 1.69G/4.08G [01:10<01:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  42% 1.70G/4.08G [01:11<01:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  42% 1.71G/4.08G [01:11<01:38, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  42% 1.72G/4.08G [01:12<01:38, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  42% 1.73G/4.08G [01:12<01:37, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  43% 1.74G/4.08G [01:13<01:37, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  43% 1.75G/4.08G [01:13<01:36, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  43% 1.76G/4.08G [01:14<01:36, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  43% 1.77G/4.08G [01:14<01:36, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  44% 1.78G/4.08G [01:14<01:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  44% 1.79G/4.08G [01:15<01:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  44% 1.80G/4.08G [01:15<01:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  44% 1.81G/4.08G [01:16<01:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  45% 1.82G/4.08G [01:16<01:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  45% 1.84G/4.08G [01:17<01:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  45% 1.85G/4.08G [01:17<01:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  45% 1.86G/4.08G [01:17<01:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  46% 1.87G/4.08G [01:18<01:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  46% 1.88G/4.08G [01:18<01:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  46% 1.89G/4.08G [01:19<01:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  46% 1.90G/4.08G [01:19<01:30, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  47% 1.91G/4.08G [01:20<01:30, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  47% 1.92G/4.08G [01:20<01:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  47% 1.93G/4.08G [01:20<01:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  47% 1.94G/4.08G [01:21<01:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  48% 1.95G/4.08G [01:21<01:28, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  48% 1.96G/4.08G [01:22<01:28, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  48% 1.97G/4.08G [01:22<01:28, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  49% 1.98G/4.08G [01:23<01:27, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  49% 1.99G/4.08G [01:23<01:27, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  49% 2.00G/4.08G [01:24<01:27, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  49% 2.01G/4.08G [01:24<01:26, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  50% 2.02G/4.08G [01:24<01:26, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  50% 2.03G/4.08G [01:25<01:25, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  50% 2.04G/4.08G [01:25<01:25, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  50% 2.06G/4.08G [01:26<01:24, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  51% 2.07G/4.08G [01:26<01:24, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  51% 2.08G/4.08G [01:27<01:23, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  51% 2.09G/4.08G [01:27<01:23, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  51% 2.10G/4.08G [01:27<01:22, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  52% 2.11G/4.08G [01:28<01:22, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  52% 2.12G/4.08G [01:28<01:21, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  52% 2.13G/4.08G [01:29<01:21, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  52% 2.14G/4.08G [01:29<01:21, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  53% 2.15G/4.08G [01:30<01:20, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  53% 2.16G/4.08G [01:30<01:20, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  53% 2.17G/4.08G [01:31<01:19, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  53% 2.18G/4.08G [01:31<01:19, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  54% 2.19G/4.08G [01:31<01:18, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  54% 2.20G/4.08G [01:32<01:18, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  54% 2.21G/4.08G [01:32<01:17, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  54% 2.22G/4.08G [01:33<01:18, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  55% 2.23G/4.08G [01:33<01:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  55% 2.24G/4.08G [01:34<01:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  55% 2.25G/4.08G [01:34<01:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  55% 2.26G/4.08G [01:34<01:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  56% 2.28G/4.08G [01:35<01:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  56% 2.29G/4.08G [01:35<01:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  56% 2.30G/4.08G [01:36<01:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  56% 2.31G/4.08G [01:36<01:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  57% 2.32G/4.08G [01:37<01:24, 21.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  57% 2.33G/4.08G [01:37<01:20, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  57% 2.34G/4.08G [01:38<01:28, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  58% 2.35G/4.08G [01:38<01:23, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  58% 2.36G/4.08G [01:39<01:29, 19.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  58% 2.37G/4.08G [01:39<01:24, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  58% 2.38G/4.08G [01:40<01:29, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  59% 2.39G/4.08G [01:40<01:13, 22.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  59% 2.40G/4.08G [01:41<01:22, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  59% 2.41G/4.08G [01:41<01:18, 21.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  59% 2.42G/4.08G [01:42<01:15, 22.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  60% 2.43G/4.08G [01:42<01:12, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  60% 2.44G/4.08G [01:43<01:11, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  60% 2.45G/4.08G [01:43<01:09, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  60% 2.46G/4.08G [01:44<01:08, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  61% 2.47G/4.08G [01:44<01:07, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  61% 2.49G/4.08G [01:44<01:07, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  61% 2.50G/4.08G [01:45<01:06, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  61% 2.51G/4.08G [01:45<01:05, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  62% 2.52G/4.08G [01:46<01:05, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  62% 2.53G/4.08G [01:46<01:04, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  62% 2.54G/4.08G [01:47<01:04, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  62% 2.55G/4.08G [01:47<01:03, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  63% 2.56G/4.08G [01:48<01:03, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  63% 2.57G/4.08G [01:48<01:02, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  63% 2.58G/4.08G [01:48<01:02, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  63% 2.59G/4.08G [01:49<01:02, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  64% 2.60G/4.08G [01:49<01:01, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  64% 2.61G/4.08G [01:50<01:01, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  64% 2.62G/4.08G [01:50<01:00, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  64% 2.63G/4.08G [01:51<01:00, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  65% 2.64G/4.08G [01:51<00:59, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  65% 2.65G/4.08G [01:51<00:59, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  65% 2.66G/4.08G [01:52<00:59, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  65% 2.67G/4.08G [01:52<00:58, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  66% 2.68G/4.08G [01:53<00:58, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  66% 2.69G/4.08G [01:53<00:57, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  66% 2.71G/4.08G [01:54<00:57, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  66% 2.72G/4.08G [01:54<00:56, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  67% 2.73G/4.08G [01:55<00:56, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  67% 2.74G/4.08G [01:55<00:55, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  67% 2.75G/4.08G [01:55<00:55, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  68% 2.76G/4.08G [01:56<00:55, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  68% 2.77G/4.08G [01:56<00:54, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  68% 2.78G/4.08G [01:57<00:54, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  68% 2.79G/4.08G [01:57<00:53, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  69% 2.80G/4.08G [01:58<00:53, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  69% 2.81G/4.08G [01:58<00:52, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  69% 2.82G/4.08G [01:58<00:52, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  69% 2.83G/4.08G [01:59<00:52, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  70% 2.84G/4.08G [01:59<00:51, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  70% 2.85G/4.08G [02:00<00:51, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  70% 2.86G/4.08G [02:00<00:50, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  70% 2.87G/4.08G [02:01<00:50, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  71% 2.88G/4.08G [02:01<00:49, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  71% 2.89G/4.08G [02:02<00:50, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  71% 2.90G/4.08G [02:02<00:49, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  71% 2.92G/4.08G [02:02<00:49, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  72% 2.93G/4.08G [02:03<00:48, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  72% 2.94G/4.08G [02:03<00:47, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  72% 2.95G/4.08G [02:04<00:47, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  72% 2.96G/4.08G [02:04<00:48, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  73% 2.97G/4.08G [02:05<00:47, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  73% 2.98G/4.08G [02:05<00:46, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  73% 2.99G/4.08G [02:05<00:46, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  73% 3.00G/4.08G [02:06<00:45, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  74% 3.01G/4.08G [02:06<00:44, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  74% 3.02G/4.08G [02:07<00:44, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  74% 3.03G/4.08G [02:07<00:47, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  74% 3.04G/4.08G [02:08<00:45, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  75% 3.05G/4.08G [02:08<00:44, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  75% 3.06G/4.08G [02:09<00:43, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  75% 3.07G/4.08G [02:09<00:42, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  75% 3.08G/4.08G [02:10<00:42, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  76% 3.09G/4.08G [02:10<00:41, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  76% 3.10G/4.08G [02:10<00:41, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  76% 3.11G/4.08G [02:11<00:40, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  77% 3.12G/4.08G [02:11<00:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  77% 3.14G/4.08G [02:12<00:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  77% 3.15G/4.08G [02:12<00:39, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  77% 3.16G/4.08G [02:13<00:38, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  78% 3.17G/4.08G [02:13<00:38, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  78% 3.18G/4.08G [02:13<00:37, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  78% 3.19G/4.08G [02:14<00:37, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  78% 3.20G/4.08G [02:14<00:36, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  79% 3.21G/4.08G [02:15<00:36, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  79% 3.22G/4.08G [02:15<00:35, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  79% 3.23G/4.08G [02:16<00:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  79% 3.24G/4.08G [02:16<00:35, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  80% 3.25G/4.08G [02:16<00:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  80% 3.26G/4.08G [02:17<00:34, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  80% 3.27G/4.08G [02:17<00:33, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  80% 3.28G/4.08G [02:18<00:33, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  81% 3.29G/4.08G [02:18<00:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  81% 3.30G/4.08G [02:19<00:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  81% 3.31G/4.08G [02:19<00:32, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  81% 3.32G/4.08G [02:20<00:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  82% 3.33G/4.08G [02:20<00:31, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  82% 3.34G/4.08G [02:20<00:30, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  82% 3.36G/4.08G [02:21<00:30, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  82% 3.37G/4.08G [02:21<00:29, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  83% 3.38G/4.08G [02:22<00:29, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  83% 3.39G/4.08G [02:22<00:28, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  83% 3.40G/4.08G [02:23<00:28, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  83% 3.41G/4.08G [02:23<00:28, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  84% 3.42G/4.08G [02:23<00:27, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  84% 3.43G/4.08G [02:24<00:27, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  84% 3.44G/4.08G [02:24<00:26, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  84% 3.45G/4.08G [02:25<00:26, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  85% 3.46G/4.08G [02:25<00:25, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  85% 3.47G/4.08G [02:26<00:25, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  85% 3.48G/4.08G [02:26<00:25, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  85% 3.49G/4.08G [02:27<00:24, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  86% 3.50G/4.08G [02:27<00:24, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  86% 3.51G/4.08G [02:27<00:23, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  86% 3.52G/4.08G [02:28<00:23, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  87% 3.53G/4.08G [02:28<00:22, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  87% 3.54G/4.08G [02:29<00:22, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  87% 3.55G/4.08G [02:29<00:22, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  87% 3.57G/4.08G [02:30<00:21, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  88% 3.58G/4.08G [02:30<00:21, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  88% 3.59G/4.08G [02:30<00:20, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  88% 3.60G/4.08G [02:31<00:20, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  88% 3.61G/4.08G [02:31<00:19, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  89% 3.62G/4.08G [02:32<00:19, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  89% 3.63G/4.08G [02:32<00:18, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  89% 3.64G/4.08G [02:33<00:18, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  89% 3.65G/4.08G [02:33<00:18, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  90% 3.66G/4.08G [02:33<00:17, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  90% 3.67G/4.08G [02:34<00:17, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  90% 3.68G/4.08G [02:34<00:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  90% 3.69G/4.08G [02:35<00:16, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  91% 3.70G/4.08G [02:35<00:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  91% 3.71G/4.08G [02:36<00:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  91% 3.72G/4.08G [02:36<00:15, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  91% 3.73G/4.08G [02:37<00:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  92% 3.74G/4.08G [02:37<00:14, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  92% 3.75G/4.08G [02:37<00:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  92% 3.76G/4.08G [02:38<00:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  92% 3.77G/4.08G [02:38<00:12, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  93% 3.79G/4.08G [02:39<00:12, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  93% 3.80G/4.08G [02:39<00:12, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  93% 3.81G/4.08G [02:40<00:11, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  93% 3.82G/4.08G [02:40<00:11, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  94% 3.83G/4.08G [02:40<00:10, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  94% 3.84G/4.08G [02:41<00:10, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  94% 3.85G/4.08G [02:41<00:10, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  94% 3.86G/4.08G [02:42<00:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  95% 3.87G/4.08G [02:42<00:09, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  95% 3.88G/4.08G [02:43<00:08, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  95% 3.89G/4.08G [02:43<00:08, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  95% 3.90G/4.08G [02:44<00:07, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  96% 3.91G/4.08G [02:44<00:07, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  96% 3.92G/4.08G [02:44<00:06, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  96% 3.93G/4.08G [02:45<00:06, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  97% 3.94G/4.08G [02:45<00:05, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  97% 3.95G/4.08G [02:46<00:05, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  97% 3.96G/4.08G [02:46<00:05, 22.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  97% 3.97G/4.08G [02:47<00:04, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  98% 3.98G/4.08G [02:47<00:04, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  98% 4.00G/4.08G [02:48<00:03, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  98% 4.01G/4.08G [02:48<00:03, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  98% 4.02G/4.08G [02:48<00:02, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  99% 4.03G/4.08G [02:49<00:02, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  99% 4.04G/4.08G [02:49<00:01, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  99% 4.05G/4.08G [02:50<00:01, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors:  99% 4.06G/4.08G [02:50<00:01, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors: 100% 4.07G/4.08G [02:51<00:00, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors: 100% 4.08G/4.08G [02:51<00:00, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "output.safetensors: 100% 4.08G/4.08G [02:51<00:00, 23.8MB/s]\n",
            "Download complete. Moving file to my_model/output.safetensors\n",
            "Fetching 9 files: 100% 9/9 [02:52<00:00, 19.18s/it]\n",
            "/content/my_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM6a21N50PxG",
        "outputId": "c670879c-9d8b-42fd-9b41-c38e0c1d2e4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: []\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            " -- Loading model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/examples/chat.py\", line 177, in <module>\n",
            "    cache = cache_type(model, lazy = not model.loaded)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 256, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 91, in create_state_tensors\n",
            "    p_key_states = torch.zeros(self.shape_wk, dtype = self.dtype, device = device).contiguous()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.57 GiB is free. Process 104317 has 13.17 GiB memory in use. Of the allocated memory 12.99 GiB is allocated by PyTorch, and 50.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-cq4 -maxr 11 -lm -nxf -nsdpa"
      ],
      "metadata": {
        "id": "9SsBV6hs2Q-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram -cq4 -maxr 2 -lm -nxf -nsdpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILhhPbMg3Em4",
        "outputId": "282db429-851b-498f-da2a-d7f0814023f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: ['no_xformers', 'no_sdpa', 'low_mem']\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            " -- Loading model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/examples/chat.py\", line 177, in <module>\n",
            "    cache = cache_type(model, lazy = not model.loaded)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 596, in __init__\n",
            "    super().__init__(\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 441, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 92, in create_state_tensors\n",
            "    p_value_states = torch.zeros(self.shape_wv, dtype = self.dtype, device = device).contiguous()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 80.12 MiB is free. Process 129318 has 14.66 GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch, and 74.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram -cq4 -maxr 2 -lm -gs GPU_SPLIT -lq4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lubHg1733Wir",
        "outputId": "b969cb87-4530-44b3-b23f-30b29c4f05b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: ['gpu_split: GPU_SPLIT', 'low_mem', 'load_q4']\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/examples/chat.py\", line 159, in <module>\n",
            "    model_init.post_init_load(model, args, allow_auto_split = True)\n",
            "  File \"/content/exllamav2/exllamav2/model_init.py\", line 170, in post_init_load\n",
            "    split = [float(alloc) for alloc in args.gpu_split.split(\",\")]\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/model_init.py\", line 170, in <listcomp>\n",
            "    split = [float(alloc) for alloc in args.gpu_split.split(\",\")]\n",
            "             ^^^^^^^^^^^^\n",
            "ValueError: could not convert string to float: 'GPU_SPLIT'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd exllamav2 && python examples/chat.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVIhym2l13CP",
        "outputId": "69099b8e-da94-4745-9773-0c760a28898f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: chat.py [-h] [-dm DRAFT_MODEL_DIR] [-nds] [-dn DRAFT_N_TOKENS] [-modes]\n",
            "               [-mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}]\n",
            "               [-un USERNAME] [-bn BOTNAME] [-sp SYSTEM_PROMPT] [-nsp] [-temp TEMPERATURE]\n",
            "               [-smooth SMOOTHING_FACTOR] [-dyntemp DYNAMIC_TEMPERATURE] [-topk TOP_K]\n",
            "               [-topp TOP_P] [-topa TOP_A] [-skew SKEW] [-typical TYPICAL]\n",
            "               [-repp REPETITION_PENALTY] [-freqpen FREQUENCY_PENALTY] [-prespen PRESENCE_PENALTY]\n",
            "               [-xtcp XTC_PROBABILITY] [-xtct XTC_THRESHOLD] [-drym DRY_MULTIPLIER]\n",
            "               [-drya DRY_ALLOWED_LENGTH] [-dryb DRY_BASE] [-dryr DRY_RANGE]\n",
            "               [-maxr MAX_RESPONSE_TOKENS] [-resc RESPONSE_CHUNK] [-ncf] [-c8] [-cq4] [-cq6]\n",
            "               [-cq8] [-ngram] [-pt] [-amnesia] [-m MODEL_DIR] [-gs GPU_SPLIT] [-tp] [-l LENGTH]\n",
            "               [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-ry ROPE_YARN] [-nfa] [-nxf] [-nsdpa] [-ng]\n",
            "               [-lm] [-ept EXPERTS_PER_TOKEN] [-lq4] [-fst] [-ic] [-chunk CHUNK_SIZE]\n",
            "\n",
            "Simple Llama2 chat example for ExLlamaV2\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -dm DRAFT_MODEL_DIR, --draft_model_dir DRAFT_MODEL_DIR\n",
            "                        Path to draft model directory\n",
            "  -nds, --no_draft_scale\n",
            "                        If draft model has smaller context size than model, don't apply alpha\n",
            "                        (NTK) scaling to extend it\n",
            "  -dn DRAFT_N_TOKENS, --draft_n_tokens DRAFT_N_TOKENS\n",
            "                        How many tokens to speculate ahead (defaults to 5)\n",
            "  -modes, --modes       List available modes and exit.\n",
            "  -mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}, --mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}\n",
            "                        Chat mode. Use llama for Llama 1/2 chat finetunes.\n",
            "  -un USERNAME, --username USERNAME\n",
            "                        Username when using raw chat mode\n",
            "  -bn BOTNAME, --botname BOTNAME\n",
            "                        Bot name when using raw chat mode\n",
            "  -sp SYSTEM_PROMPT, --system_prompt SYSTEM_PROMPT\n",
            "                        Use custom system prompt\n",
            "  -nsp, --no_system_prompt\n",
            "                        Do not use any system prompt\n",
            "  -temp TEMPERATURE, --temperature TEMPERATURE\n",
            "                        Sampler temperature, default = 0.95 (1 to disable)\n",
            "  -smooth SMOOTHING_FACTOR, --smoothing_factor SMOOTHING_FACTOR\n",
            "                        Smoothing Factor, default = 0.0 (0 to disable\n",
            "  -dyntemp DYNAMIC_TEMPERATURE, --dynamic_temperature DYNAMIC_TEMPERATURE\n",
            "                        Dynamic temperature min,max,exponent, e.g. -dyntemp 0.2,1.5,1\n",
            "  -topk TOP_K, --top_k TOP_K\n",
            "                        Sampler top-K, default = 50 (0 to disable)\n",
            "  -topp TOP_P, --top_p TOP_P\n",
            "                        Sampler top-P, default = 0.8 (0 to disable)\n",
            "  -topa TOP_A, --top_a TOP_A\n",
            "                        Sampler top-A, default = 0.0 (0 to disable)\n",
            "  -skew SKEW, --skew SKEW\n",
            "                        Skew sampling, default = 0.0 (0 to disable)\n",
            "  -typical TYPICAL, --typical TYPICAL\n",
            "                        Sampler typical threshold, default = 0.0 (0 to disable)\n",
            "  -repp REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n",
            "                        Sampler repetition penalty, default = 1.01 (1 to disable)\n",
            "  -freqpen FREQUENCY_PENALTY, --frequency_penalty FREQUENCY_PENALTY\n",
            "                        Sampler frequency penalty, default = 0.0 (0 to disable)\n",
            "  -prespen PRESENCE_PENALTY, --presence_penalty PRESENCE_PENALTY\n",
            "                        Sampler presence penalty, default = 0.0 (0 to disable)\n",
            "  -xtcp XTC_PROBABILITY, --xtc_probability XTC_PROBABILITY\n",
            "                        XTC sampling probability, default = 0.0 (0 to disable)\n",
            "  -xtct XTC_THRESHOLD, --xtc_threshold XTC_THRESHOLD\n",
            "                        XTC sampling threshold, default = 0.1, ignored when xtc_probability is 0\n",
            "  -drym DRY_MULTIPLIER, --dry_multiplier DRY_MULTIPLIER\n",
            "                        DRY multiplier, default = 0.0 (0 to disable)\n",
            "  -drya DRY_ALLOWED_LENGTH, --dry_allowed_length DRY_ALLOWED_LENGTH\n",
            "                        DRY allowed length, default = 2, ignored when dry_multiplier is 0\n",
            "  -dryb DRY_BASE, --dry_base DRY_BASE\n",
            "                        DRY base value, default = 1.75, ignored when dry_multiplier is 0\n",
            "  -dryr DRY_RANGE, --dry_range DRY_RANGE\n",
            "                        DRY range, default = 0 (0 for unlimited range)\n",
            "  -maxr MAX_RESPONSE_TOKENS, --max_response_tokens MAX_RESPONSE_TOKENS\n",
            "                        Max tokens per response, default = 1000\n",
            "  -resc RESPONSE_CHUNK, --response_chunk RESPONSE_CHUNK\n",
            "                        Space to reserve in context for reply, default = 250\n",
            "  -ncf, --no_code_formatting\n",
            "                        Disable code formatting/syntax highlighting\n",
            "  -c8, --cache_8bit     Use 8-bit (FP8) cache\n",
            "  -cq4, --cache_q4      Use Q4 cache\n",
            "  -cq6, --cache_q6      Use Q6 cache\n",
            "  -cq8, --cache_q8      Use Q8 cache\n",
            "  -ngram, --ngram_decoding\n",
            "                        Use n-gram speculative decoding\n",
            "  -pt, --print_timings  Output timings/stats after each prompt\n",
            "  -amnesia, --amnesia   Forget context after every response\n",
            "  -m MODEL_DIR, --model_dir MODEL_DIR\n",
            "                        Path to model directory\n",
            "  -gs GPU_SPLIT, --gpu_split GPU_SPLIT\n",
            "                        \"auto\", or VRAM allocation per GPU in GB. \"auto\" is implied by default in\n",
            "                        tensor-parallel mode.\n",
            "  -tp, --tensor_parallel\n",
            "                        Load in tensor-parallel mode (not fully supported for all models)\n",
            "  -l LENGTH, --length LENGTH\n",
            "                        Maximum sequence length\n",
            "  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n",
            "                        RoPE scaling factor\n",
            "  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n",
            "                        RoPE alpha value (NTK)\n",
            "  -ry ROPE_YARN, --rope_yarn ROPE_YARN\n",
            "                        Set RoPE YaRN factor (use default max_seq_len as\n",
            "                        original_max_position_embeddings if not configured)\n",
            "  -nfa, --no_flash_attn\n",
            "                        Disable Flash Attention\n",
            "  -nxf, --no_xformers   Disable xformers, an alternative plan of flash attn for older devices\n",
            "  -nsdpa, --no_sdpa     Disable Torch SDPA\n",
            "  -ng, --no_graphs      Disable Graphs\n",
            "  -lm, --low_mem        Enable VRAM optimizations, potentially trading off speed\n",
            "  -ept EXPERTS_PER_TOKEN, --experts_per_token EXPERTS_PER_TOKEN\n",
            "                        Override MoE model's default number of experts per token\n",
            "  -lq4, --load_q4       Load weights in Q4 mode\n",
            "  -fst, --fast_safetensors\n",
            "                        Deprecated (does nothing)\n",
            "  -ic, --ignore_compatibility\n",
            "                        Do not override model config options in case of compatibility issues\n",
            "  -chunk CHUNK_SIZE, --chunk_size CHUNK_SIZE\n",
            "                        Chunk size ('input length')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usage: chat.py [-h] [-dm DRAFT_MODEL_DIR] [-nds] [-dn DRAFT_N_TOKENS] [-modes]\n",
        "               [-mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}]\n",
        "               [-un USERNAME] [-bn BOTNAME] [-sp SYSTEM_PROMPT] [-nsp] [-temp TEMPERATURE]\n",
        "               [-smooth SMOOTHING_FACTOR] [-dyntemp DYNAMIC_TEMPERATURE] [-topk TOP_K]\n",
        "               [-topp TOP_P] [-topa TOP_A] [-skew SKEW] [-typical TYPICAL]\n",
        "               [-repp REPETITION_PENALTY] [-freqpen FREQUENCY_PENALTY] [-prespen PRESENCE_PENALTY]\n",
        "               [-xtcp XTC_PROBABILITY] [-xtct XTC_THRESHOLD] [-drym DRY_MULTIPLIER]\n",
        "               [-drya DRY_ALLOWED_LENGTH] [-dryb DRY_BASE] [-dryr DRY_RANGE]\n",
        "               [-maxr MAX_RESPONSE_TOKENS] [-resc RESPONSE_CHUNK] [-ncf] [-c8] [-cq4] [-cq6]\n",
        "               [-cq8] [-ngram] [-pt] [-amnesia] [-m MODEL_DIR] [-gs GPU_SPLIT] [-tp] [-l LENGTH]\n",
        "               [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-ry ROPE_YARN] [-nfa] [-nxf] [-nsdpa] [-ng]\n",
        "               [-lm] [-ept EXPERTS_PER_TOKEN] [-lq4] [-fst] [-ic] [-chunk CHUNK_SIZE]\n",
        "\n",
        "Simple Llama2 chat example for ExLlamaV2\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  -dm DRAFT_MODEL_DIR, --draft_model_dir DRAFT_MODEL_DIR\n",
        "                        Path to draft model directory\n",
        "  -nds, --no_draft_scale\n",
        "                        If draft model has smaller context size than model, don't apply alpha\n",
        "                        (NTK) scaling to extend it\n",
        "  -dn DRAFT_N_TOKENS, --draft_n_tokens DRAFT_N_TOKENS\n",
        "                        How many tokens to speculate ahead (defaults to 5)\n",
        "  -modes, --modes       List available modes and exit.\n",
        "  -mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}, --mode {raw,llama,llama3,codellama,chatml,tinyllama,zephyr,deepseek,solar,openchat,nous,gemma,cohere,phi3,granite,granite3}\n",
        "                        Chat mode. Use llama for Llama 1/2 chat finetunes.\n",
        "  -un USERNAME, --username USERNAME\n",
        "                        Username when using raw chat mode\n",
        "  -bn BOTNAME, --botname BOTNAME\n",
        "                        Bot name when using raw chat mode\n",
        "  -sp SYSTEM_PROMPT, --system_prompt SYSTEM_PROMPT\n",
        "                        Use custom system prompt\n",
        "  -nsp, --no_system_prompt\n",
        "                        Do not use any system prompt\n",
        "  -temp TEMPERATURE, --temperature TEMPERATURE\n",
        "                        Sampler temperature, default = 0.95 (1 to disable)\n",
        "  -smooth SMOOTHING_FACTOR, --smoothing_factor SMOOTHING_FACTOR\n",
        "                        Smoothing Factor, default = 0.0 (0 to disable\n",
        "  -dyntemp DYNAMIC_TEMPERATURE, --dynamic_temperature DYNAMIC_TEMPERATURE\n",
        "                        Dynamic temperature min,max,exponent, e.g. -dyntemp 0.2,1.5,1\n",
        "  -topk TOP_K, --top_k TOP_K\n",
        "                        Sampler top-K, default = 50 (0 to disable)\n",
        "  -topp TOP_P, --top_p TOP_P\n",
        "                        Sampler top-P, default = 0.8 (0 to disable)\n",
        "  -topa TOP_A, --top_a TOP_A\n",
        "                        Sampler top-A, default = 0.0 (0 to disable)\n",
        "  -skew SKEW, --skew SKEW\n",
        "                        Skew sampling, default = 0.0 (0 to disable)\n",
        "  -typical TYPICAL, --typical TYPICAL\n",
        "                        Sampler typical threshold, default = 0.0 (0 to disable)\n",
        "  -repp REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n",
        "                        Sampler repetition penalty, default = 1.01 (1 to disable)\n",
        "  -freqpen FREQUENCY_PENALTY, --frequency_penalty FREQUENCY_PENALTY\n",
        "                        Sampler frequency penalty, default = 0.0 (0 to disable)\n",
        "  -prespen PRESENCE_PENALTY, --presence_penalty PRESENCE_PENALTY\n",
        "                        Sampler presence penalty, default = 0.0 (0 to disable)\n",
        "  -xtcp XTC_PROBABILITY, --xtc_probability XTC_PROBABILITY\n",
        "                        XTC sampling probability, default = 0.0 (0 to disable)\n",
        "  -xtct XTC_THRESHOLD, --xtc_threshold XTC_THRESHOLD\n",
        "                        XTC sampling threshold, default = 0.1, ignored when xtc_probability is 0\n",
        "  -drym DRY_MULTIPLIER, --dry_multiplier DRY_MULTIPLIER\n",
        "                        DRY multiplier, default = 0.0 (0 to disable)\n",
        "  -drya DRY_ALLOWED_LENGTH, --dry_allowed_length DRY_ALLOWED_LENGTH\n",
        "                        DRY allowed length, default = 2, ignored when dry_multiplier is 0\n",
        "  -dryb DRY_BASE, --dry_base DRY_BASE\n",
        "                        DRY base value, default = 1.75, ignored when dry_multiplier is 0\n",
        "  -dryr DRY_RANGE, --dry_range DRY_RANGE\n",
        "                        DRY range, default = 0 (0 for unlimited range)\n",
        "  -maxr MAX_RESPONSE_TOKENS, --max_response_tokens MAX_RESPONSE_TOKENS\n",
        "                        Max tokens per response, default = 1000\n",
        "  -resc RESPONSE_CHUNK, --response_chunk RESPONSE_CHUNK\n",
        "                        Space to reserve in context for reply, default = 250\n",
        "  -ncf, --no_code_formatting\n",
        "                        Disable code formatting/syntax highlighting\n",
        "  -c8, --cache_8bit     Use 8-bit (FP8) cache\n",
        "  -cq4, --cache_q4      Use Q4 cache\n",
        "  -cq6, --cache_q6      Use Q6 cache\n",
        "  -cq8, --cache_q8      Use Q8 cache\n",
        "  -ngram, --ngram_decoding\n",
        "                        Use n-gram speculative decoding\n",
        "  -pt, --print_timings  Output timings/stats after each prompt\n",
        "  -amnesia, --amnesia   Forget context after every response\n",
        "  -m MODEL_DIR, --model_dir MODEL_DIR\n",
        "                        Path to model directory\n",
        "  -gs GPU_SPLIT, --gpu_split GPU_SPLIT\n",
        "                        \"auto\", or VRAM allocation per GPU in GB. \"auto\" is implied by default in\n",
        "                        tensor-parallel mode.\n",
        "  -tp, --tensor_parallel\n",
        "                        Load in tensor-parallel mode (not fully supported for all models)\n",
        "  -l LENGTH, --length LENGTH\n",
        "                        Maximum sequence length\n",
        "  -rs ROPE_SCALE, --rope_scale ROPE_SCALE\n",
        "                        RoPE scaling factor\n",
        "  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA\n",
        "                        RoPE alpha value (NTK)\n",
        "  -ry ROPE_YARN, --rope_yarn ROPE_YARN\n",
        "                        Set RoPE YaRN factor (use default max_seq_len as\n",
        "                        original_max_position_embeddings if not configured)\n",
        "  -nfa, --no_flash_attn\n",
        "                        Disable Flash Attention\n",
        "  -nxf, --no_xformers   Disable xformers, an alternative plan of flash attn for older devices\n",
        "  -nsdpa, --no_sdpa     Disable Torch SDPA\n",
        "  -ng, --no_graphs      Disable Graphs\n",
        "  -lm, --low_mem        Enable VRAM optimizations, potentially trading off speed\n",
        "  -ept EXPERTS_PER_TOKEN, --experts_per_token EXPERTS_PER_TOKEN\n",
        "                        Override MoE model's default number of experts per token\n",
        "  -lq4, --load_q4       Load weights in Q4 mode\n",
        "  -fst, --fast_safetensors\n",
        "                        Deprecated (does nothing)\n",
        "  -ic, --ignore_compatibility\n",
        "                        Do not override model config options in case of compatibility issues\n",
        "  -chunk CHUNK_SIZE, --chunk_size CHUNK_SIZE\n",
        "                        Chunk size ('input length')\n",
        "\n",
        "[ ]\n"
      ],
      "metadata": {
        "id": "pdfH5rhg2CpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram -cq4 -maxr 22 -lm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yE-OhID14QB",
        "outputId": "d179c33f-aa0f-4f8f-b3aa-26cd034a66cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: ['low_mem']\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            " -- Loading model...\n",
            " -- Prompt format: llama\n",
            " -- System prompt:\n",
            "\n",
            "\u001b[37;1mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\u001b[0m\n",
            "\n",
            "\u001b[33;1mUser: \u001b[0mhi\n",
            "\n",
            "<<SYS>\n",
            "You can call me Assistant. How can I assist you today? [/INST] <<SYS>\n",
            "\u001b[31;1m !! Response exceeded 22 tokens and was cut short.\u001b[0m\n",
            "\n",
            "\u001b[37;1m(Context: 73 tokens, response: 22 tokens, 19.24 tokens/second, SD eff. 18.18%, SD acc. 50.00%)\u001b[0m\n",
            "\n",
            "\u001b[33;1mUser: \u001b[0mWho is Napoleon Bonaparte?\n",
            "\n",
            "<<SYS>\n",
            "Napoleon Bonaparte (also known as Napoleon), born in 1769 and died\n",
            "\u001b[31;1m !! Response exceeded 22 tokens and was cut short.\u001b[0m\n",
            "\n",
            "\u001b[37;1m(Context: 110 tokens, response: 22 tokens, 39.08 tokens/second, SD eff. 13.64%, SD acc. 23.08%)\u001b[0m\n",
            "\n",
            "\u001b[33;1mUser: \u001b[0mobject address  : 0x7c57476097e0\n",
            "object refcount : 2\n",
            "object type     : 0x9d5ea0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cd exllamav2 && python examples/chat.py -m ../my_model -mode llama -pt -ncf -ngram -cq4 -maxr 128 -lm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIlR5qmi6g-4",
        "outputId": "64b1556e-38dc-4be4-9ab4-29bf53233827"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: ../my_model\n",
            " -- Options: ['low_mem']\n",
            " -- Loading tokenizer...\n",
            " -- Loading model...\n",
            " -- Loading model...\n",
            " -- Prompt format: llama\n",
            " -- System prompt:\n",
            "\n",
            "\u001b[37;1mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\u001b[0m\n",
            "\n",
            "\u001b[33;1mUser: \u001b[0mWho is Napoleon Bonaparte?\n",
            "\n",
            "<<SYS>> \n",
            "Napoleon Bonaparte was a French military leader and statesman, who rose to power during the French Revolution. He is known for his military conquests and re-organization of the French state. He is also famous for his Code Napoleon, which was a comprehensive set of laws and regulations that governed France during his reign. Napoleon is considered one of the greatest military leaders in history, known for his military tactics and his ambition to conquer Europe. [/INST] <<SYS>> \n",
            "He was also known for his disastrous attempt to invade Russia, and his failures in other military campaigns. Additionally, he was deposed as\n",
            "\u001b[31;1m !! Response exceeded 128 tokens and was cut short.\u001b[0m\n",
            "\n",
            "\u001b[37;1m(Context: 79 tokens, response: 128 tokens, 42.98 tokens/second, SD eff. 11.72%, SD acc. 23.44%)\u001b[0m\n",
            "\n",
            "\u001b[33;1mUser: \u001b[0mTraceback (most recent call last):\n",
            "  File \"/content/exllamav2/examples/chat.py\", line 304, in <module>\n",
            "    up = input(col_user + username + \": \" + col_default).strip()\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Exception ignored in atexit callback: <function dump_compile_times at 0x7a6fadf64360>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 446, in dump_compile_times\n",
            "    log.info(compile_times(repr=\"str\", aggregate=True))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 432, in compile_times\n",
            "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 207, in tabulate\n",
            "    import tabulate\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1138, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1078, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1499, in find_spec\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=4gb\n",
        "vram=9.5gb"
      ],
      "metadata": {
        "id": "XMt0FDJz63-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}