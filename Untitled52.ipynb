{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHLEqh84Dzs0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxzSD5pyUrsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/turboderp/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kbBYI4uUru8",
        "outputId": "22d21779-15bd-4c33-fb5d-2b2df0e9dfb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 8145, done.\u001b[K\n",
            "remote: Counting objects: 100% (3373/3373), done.\u001b[K\n",
            "remote: Compressing objects: 100% (971/971), done.\u001b[K\n",
            "remote: Total 8145 (delta 2474), reused 2430 (delta 2402), pack-reused 4772 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8145/8145), 21.93 MiB | 16.12 MiB/s, done.\n",
            "Resolving deltas: 100% (5872/5872), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZHVUz49UuFh",
        "outputId": "335d5905-182b-4ed6-984e-043c3f4a960f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting exllamav2\n",
            "  Downloading exllamav2-0.2.8-py3-none-any.whl.metadata (467 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.2.2)\n",
            "Collecting ninja (from exllamav2)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting fastparquet (from exllamav2)\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from exllamav2) (14.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2024.11.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from exllamav2) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from exllamav2) (13.9.4)\n",
            "Requirement already satisfied: pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from exllamav2) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.2.0->exllamav2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->exllamav2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->exllamav2) (1.3.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->exllamav2) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->exllamav2) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->exllamav2) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->exllamav2) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->exllamav2) (3.0.2)\n",
            "Downloading exllamav2-0.2.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet, exllamav2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed exllamav2-0.2.8 fastparquet-2024.11.0 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJC_EuPlUx9i",
        "outputId": "908c4760-f606-4b6f-cdd1-f13c38cb0e8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRYmKCBpUzkI",
        "outputId": "28ab49c1-c246-42f9-a1a3-2e4466dc4f8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  3.0bpw https://huggingface.co/turboderp/gemma-3-27b-it-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJwdVgdCU2PZ",
        "outputId": "61d7c10f-8304-40c3-877e-d4252ed10dab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gemma-3-27b-it-exl2'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 49 (delta 4), reused 0 (delta 0), pack-reused 31 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (49/49), 160.87 KiB | 1.06 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 5.43 GiB | 5.47 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\toutput-00002-of-00002.safetensors\n",
            "\toutput-00001-of-00002.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m gemma-3-27b-it-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIvwgdBFU7uq",
        "outputId": "72c16d6c-31b4-41cc-df91-43dd8b83fb92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            "Loading exllamav2_ext extension (JIT)...\n",
            "\u001b[2KBuilding C++/CUDA extension \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:13:59\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Model: gemma-3-27b-it-exl2\n",
            " -- Options: []\n",
            " !! Warning, unknown architecture: Gemma3ForConditionalGeneration\n",
            " !! Loading as LlamaForCausalLM\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 100, in <module>\n",
            "    model, tokenizer = model_init.init(\n",
            "                       ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/model_init.py\", line 98, in init\n",
            "    config.prepare()\n",
            "  File \"/content/exllamav2/exllamav2/config.py\", line 247, in prepare\n",
            "    self.vocab_size = read(read_config, int, \"vocab_size\", opt_subkey = \"text_config\")\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/config.py\", line 54, in read\n",
            "    raise ValueError(f\"Missing any of the following keys: {keys}\")\n",
            "ValueError: Missing any of the following keys: ['vocab_size', 'text_config->vocab_size']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\"vocab_size\": 256000,"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ThrFjtJHeHUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "{\n",
        "    \"architectures\": [\n",
        "        \"Gemma3ForConditionalGeneration\"\n",
        "    ],\n",
        "    \"boi_token_index\": 255999,\n",
        "    \"eoi_token_index\": 256000,\n",
        "    \"eos_token_id\": [\n",
        "        1,\n",
        "        106\n",
        "    ],\n",
        "    \"image_token_index\": 262144,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"mm_tokens_per_image\": 256,\n",
        "    \"model_type\": \"gemma3\",\n",
        "    \"vocab_size\": 256000,  // Added vocab_size\n",
        "    \"text_config\": {\n",
        "        \"head_dim\": 128,\n",
        "        \"hidden_size\": 5376,\n",
        "        \"intermediate_size\": 21504,\n",
        "        \"model_type\": \"gemma3_text\",\n",
        "        \"num_attention_heads\": 32,\n",
        "        \"num_hidden_layers\": 62,\n",
        "        \"num_key_value_heads\": 16,\n",
        "        \"query_pre_attn_scalar\": 168,\n",
        "        \"rope_scaling\": {\n",
        "            \"factor\": 8.0,\n",
        "            \"rope_type\": \"linear\"\n",
        "        },\n",
        "        \"sliding_window\": 1024\n",
        "    },\n",
        "    \"torch_dtype\": \"bfloat16\",\n",
        "    \"transformers_version\": \"4.50.0.dev0\",\n",
        "    \"vision_config\": {\n",
        "        \"hidden_size\": 1152,\n",
        "        \"image_size\": 896,\n",
        "        \"intermediate_size\": 4304,\n",
        "        \"model_type\": \"siglip_vision_model\",\n",
        "        \"num_attention_heads\": 16,\n",
        "        \"num_hidden_layers\": 27,\n",
        "        \"patch_size\": 14,\n",
        "        \"vision_use_head\": false\n",
        "    },\n",
        "    \"quantization_config\": {\n",
        "        \"quant_method\": \"exl2\",\n",
        "        \"version\": \"0.2.8\",\n",
        "        \"bits\": 3.0,\n",
        "        \"head_bits\": 6,\n",
        "        \"calibration\": {\n",
        "            \"rows\": 115,\n",
        "            \"length\": 2048,\n",
        "            \"dataset\": \"(default)\"\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f8rGlVf_eXxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m gemma-3-27b-it-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK0MV3doecQL",
        "outputId": "3b8d136d-80d0-4e28-e46f-2dfaabd40e39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: gemma-3-27b-it-exl2\n",
            " -- Options: []\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 100, in <module>\n",
            "    model, tokenizer = model_init.init(\n",
            "                       ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/model_init.py\", line 98, in init\n",
            "    config.prepare()\n",
            "  File \"/content/exllamav2/exllamav2/config.py\", line 218, in prepare\n",
            "    read_config = json.load(f)\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 293, in load\n",
            "    return loads(fp.read(),\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
            "    obj, end = self.scan_once(s, idx)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^\n",
            "json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 15 column 28 (char 338)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!rm -rf /content/exllamav2/Mistral-Nemo-Instruct-12B-exl2"
      ],
      "metadata": {
        "id": "iPw-2M5Seswd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWh923nAeuSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c453dfc1-c0fc-4488-99d4-5ba59f2a2588",
        "id": "m4y4h0fae29v"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  6.0bpw https://huggingface.co/turboderp/Mistral-Nemo-Instruct-12B-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ea79a1-66b5-4ed2-8fc8-a3b500ef0d7c",
        "id": "VArVyBvKe29x"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mistral-Nemo-Instruct-12B-exl2'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 76 (delta 25), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (76/76), 4.69 MiB | 4.47 MiB/s, done.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2/Mistral-Nemo-Instruct-12B-exl2\n",
        "!wget https://huggingface.co/turboderp/Mistral-Nemo-Instruct-12B-exl2/resolve/6.0bpw/output-00001-of-00002.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzjb1tOafSp-",
        "outputId": "0ddbb51d-b824-4114-dea8-ddf0785c2f4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2/Mistral-Nemo-Instruct-12B-exl2\n",
            "--2025-03-18 23:16:27--  https://huggingface.co/turboderp/Mistral-Nemo-Instruct-12B-exl2/resolve/6.0bpw/output-00001-of-00002.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.97, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/5b/1c/5b1c019ff2cdd01eeac37bdebb50be98f5dbe1e9abfab0a1f7c8da7d8f15b544/711b0250fa2a0827c2801850e83a62a774b889dc125061f4a2b827df529b8b15?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27output-00001-of-00002.safetensors%3B+filename%3D%22output-00001-of-00002.safetensors%22%3B&Expires=1742343387&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjM0MzM4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzViLzFjLzViMWMwMTlmZjJjZGQwMWVlYWMzN2JkZWJiNTBiZTk4ZjVkYmUxZTlhYmZhYjBhMWY3YzhkYTdkOGYxNWI1NDQvNzExYjAyNTBmYTJhMDgyN2MyODAxODUwZTgzYTYyYTc3NGI4ODlkYzEyNTA2MWY0YTJiODI3ZGY1MjliOGIxNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Dyg3pcoujYEHetEQYIdlfK4B4X71oJyFf34Oyv8kEg3S3ghItJGa0jbkX7z%7EGm4tPkFXgDqc8zGyWEEfmJ9vH263dyAGiEELoBz1DCOXr3s3r6DTOfYQIb589M2I7J5OAb4u7eXzGAVxiFGvIZiWWXh2u2KGHCnFhCMZaWmVf8dCR2OPWeJeH-4sVH5FHHds94Rlq7aaKdv4uXo4eM2hCxw1F2C5knPPy58KXiEjdubUNIlUPj1%7ESv9zyjQYXozcJUZzYY2d1DTtshR1chjAe9unTpA-YbNdJWKCWUuM6%7EF2iEEfVyr6QzpN0424NofPm2hPQeCYzt8tyOUc1A1uIw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-18 23:16:27--  https://cdn-lfs-us-1.hf.co/repos/5b/1c/5b1c019ff2cdd01eeac37bdebb50be98f5dbe1e9abfab0a1f7c8da7d8f15b544/711b0250fa2a0827c2801850e83a62a774b889dc125061f4a2b827df529b8b15?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27output-00001-of-00002.safetensors%3B+filename%3D%22output-00001-of-00002.safetensors%22%3B&Expires=1742343387&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjM0MzM4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzViLzFjLzViMWMwMTlmZjJjZGQwMWVlYWMzN2JkZWJiNTBiZTk4ZjVkYmUxZTlhYmZhYjBhMWY3YzhkYTdkOGYxNWI1NDQvNzExYjAyNTBmYTJhMDgyN2MyODAxODUwZTgzYTYyYTc3NGI4ODlkYzEyNTA2MWY0YTJiODI3ZGY1MjliOGIxNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Dyg3pcoujYEHetEQYIdlfK4B4X71oJyFf34Oyv8kEg3S3ghItJGa0jbkX7z%7EGm4tPkFXgDqc8zGyWEEfmJ9vH263dyAGiEELoBz1DCOXr3s3r6DTOfYQIb589M2I7J5OAb4u7eXzGAVxiFGvIZiWWXh2u2KGHCnFhCMZaWmVf8dCR2OPWeJeH-4sVH5FHHds94Rlq7aaKdv4uXo4eM2hCxw1F2C5knPPy58KXiEjdubUNIlUPj1%7ESv9zyjQYXozcJUZzYY2d1DTtshR1chjAe9unTpA-YbNdJWKCWUuM6%7EF2iEEfVyr6QzpN0424NofPm2hPQeCYzt8tyOUc1A1uIw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.45.103, 13.33.45.57, 13.33.45.80, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.45.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8588282120 (8.0G) [binary/octet-stream]\n",
            "Saving to: ‘output-00001-of-00002.safetensors’\n",
            "\n",
            "output-00001-of-000 100%[===================>]   8.00G  23.7MB/s    in 5m 47s  \n",
            "\n",
            "2025-03-18 23:22:15 (23.6 MB/s) - ‘output-00001-of-00002.safetensors’ saved [8588282120/8588282120]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2/Mistral-Nemo-Instruct-12B-exl2\n",
        "!wget https://huggingface.co/turboderp/Mistral-Nemo-Instruct-12B-exl2/resolve/6.0bpw/output-00002-of-00002.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1abf8XHfhLc",
        "outputId": "67700458-f706-44f4-a6a1-6ca4402c58e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2/Mistral-Nemo-Instruct-12B-exl2\n",
            "--2025-03-18 23:23:02--  https://huggingface.co/turboderp/Mistral-Nemo-Instruct-12B-exl2/resolve/6.0bpw/output-00002-of-00002.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/5b/1c/5b1c019ff2cdd01eeac37bdebb50be98f5dbe1e9abfab0a1f7c8da7d8f15b544/b417748fe995bf5e37c5152d18c44d9c12f47fefd71153d8dbdd6c844bcb4e17?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27output-00002-of-00002.safetensors%3B+filename%3D%22output-00002-of-00002.safetensors%22%3B&Expires=1742343782&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjM0Mzc4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzViLzFjLzViMWMwMTlmZjJjZGQwMWVlYWMzN2JkZWJiNTBiZTk4ZjVkYmUxZTlhYmZhYjBhMWY3YzhkYTdkOGYxNWI1NDQvYjQxNzc0OGZlOTk1YmY1ZTM3YzUxNTJkMThjNDRkOWMxMmY0N2ZlZmQ3MTE1M2Q4ZGJkZDZjODQ0YmNiNGUxNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IfWH4Q9B0Ruj5ccoaC%7EnIsvJeuFN0W%7EVLMRbSWgHfX5HxfRu63hjZWQZfa%7EOdGs9UpcTKERwc3TZOMPBVV1ZIx-lTZYiddA5VFKwOExr7lreZ%7ErTiYJLm2Qkp3YaxYIqh8C5OJLmuHjBq0wHMY8pbQHvjzLosJbZjkWGuStc4W0ahRVrmTJJlwFvvyKExZgJV33WcJdmtJ96QBz6FEi-aNvN5PNiMa9syHsESEGL999rl0bBh8zK3oIKM0HysJfmTbjTEWReqpjUuEARJHLDsn%7ETkHz8TJAPK8YKS2r0QBSKrIJXna1VDAQb8TGhqEktm8ACRT-%7E-OGxW5CBJotjEQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-18 23:23:02--  https://cdn-lfs-us-1.hf.co/repos/5b/1c/5b1c019ff2cdd01eeac37bdebb50be98f5dbe1e9abfab0a1f7c8da7d8f15b544/b417748fe995bf5e37c5152d18c44d9c12f47fefd71153d8dbdd6c844bcb4e17?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27output-00002-of-00002.safetensors%3B+filename%3D%22output-00002-of-00002.safetensors%22%3B&Expires=1742343782&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjM0Mzc4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzViLzFjLzViMWMwMTlmZjJjZGQwMWVlYWMzN2JkZWJiNTBiZTk4ZjVkYmUxZTlhYmZhYjBhMWY3YzhkYTdkOGYxNWI1NDQvYjQxNzc0OGZlOTk1YmY1ZTM3YzUxNTJkMThjNDRkOWMxMmY0N2ZlZmQ3MTE1M2Q4ZGJkZDZjODQ0YmNiNGUxNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IfWH4Q9B0Ruj5ccoaC%7EnIsvJeuFN0W%7EVLMRbSWgHfX5HxfRu63hjZWQZfa%7EOdGs9UpcTKERwc3TZOMPBVV1ZIx-lTZYiddA5VFKwOExr7lreZ%7ErTiYJLm2Qkp3YaxYIqh8C5OJLmuHjBq0wHMY8pbQHvjzLosJbZjkWGuStc4W0ahRVrmTJJlwFvvyKExZgJV33WcJdmtJ96QBz6FEi-aNvN5PNiMa9syHsESEGL999rl0bBh8zK3oIKM0HysJfmTbjTEWReqpjUuEARJHLDsn%7ETkHz8TJAPK8YKS2r0QBSKrIJXna1VDAQb8TGhqEktm8ACRT-%7E-OGxW5CBJotjEQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.170.229.84, 3.170.229.54, 3.170.229.100, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.170.229.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1463201336 (1.4G) [binary/octet-stream]\n",
            "Saving to: ‘output-00002-of-00002.safetensors’\n",
            "\n",
            "output-00002-of-000 100%[===================>]   1.36G  23.3MB/s    in 61s     \n",
            "\n",
            "2025-03-18 23:24:04 (22.8 MB/s) - ‘output-00002-of-00002.safetensors’ saved [1463201336/1463201336]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m Mistral-Nemo-Instruct-12B-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446207b3-b5c0-45cb-f116-6061e6fa0730",
        "id": "-1LM7baKe29y"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: Mistral-Nemo-Instruct-12B-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-Nemo-Instruct-12B-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:01:22\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 85.7914 seconds\n",
            " -- Loading tokenizer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 192, in <module>\n",
            "    cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 256, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 92, in create_state_tensors\n",
            "    p_value_states = torch.zeros(self.shape_wv, dtype = self.dtype, device = device).contiguous()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.08 GiB is free. Process 182110 has 13.66 GiB memory in use. Of the allocated memory 13.47 GiB is allocated by PyTorch, and 64.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/exllamav2/Mistral-Nemo-Instruct-12B-exl2"
      ],
      "metadata": {
        "id": "HLhudowahXHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m Mistral-Nemo-Instruct-12B-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH7CR18Fh5KF",
        "outputId": "24544f2a-d5b0-48d4-9e9a-26c03ee8c0e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: Mistral-Nemo-Instruct-12B-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-Nemo-Instruct-12B-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:01:17\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 80.4725 seconds\n",
            " -- Loading tokenizer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 192, in <module>\n",
            "    cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 256, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 92, in create_state_tensors\n",
            "    p_value_states = torch.zeros(self.shape_wv, dtype = self.dtype, device = device).contiguous()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.08 GiB is free. Process 192660 has 13.66 GiB memory in use. Of the allocated memory 13.47 GiB is allocated by PyTorch, and 64.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYPw5Jjcktgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vixZGEXktdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m Mistral-Nemo-Instruct-12B-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH_tCVUxktZ-",
        "outputId": "b75e505c-092c-44a9-80fe-103ef7c7eb72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: Mistral-Nemo-Instruct-12B-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-Nemo-Instruct-12B-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:01:01\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 65.3542 seconds\n",
            " -- Loading tokenizer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 192, in <module>\n",
            "    cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 256, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 91, in create_state_tensors\n",
            "    p_key_states = torch.zeros(self.shape_wk, dtype = self.dtype, device = device).contiguous()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 942.12 MiB is free. Process 234904 has 13.82 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 134.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python test_inference.py -m Mistral-Nemo-Instruct-12B-exl2 -p \"Who is Napoleon Bonaparte?\" --tokens 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Zi6wtQlbE2",
        "outputId": "df97963d-81ee-47de-f765-a7e08ca26c99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            " -- Model: Mistral-Nemo-Instruct-12B-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-Nemo-Instruct-12B-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:01:01\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 64.7916 seconds\n",
            " -- Loading tokenizer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/exllamav2/test_inference.py\", line 192, in <module>\n",
            "    cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 256, in __init__\n",
            "    self.create_state_tensors(copy_from, lazy)\n",
            "  File \"/content/exllamav2/exllamav2/cache.py\", line 91, in create_state_tensors\n",
            "    p_key_states = torch.zeros(self.shape_wk, dtype = self.dtype, device = device).contiguous()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 14.74 GiB of which 942.12 MiB is free. Process 248882 has 13.82 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 134.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    }
  ]
}